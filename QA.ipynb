{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_QA (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-SQuAD-squad/QA/blob/huggingface/QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmkNAxSnaD9n"
      },
      "source": [
        "#@title Init { form-width: \"25%\" }\r\n",
        "import os\r\n",
        "import random\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import string\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "!pip install tokenizers\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "\r\n",
        "pd.set_option('display.max_colwidth', -1)\r\n",
        "\r\n",
        "# fix random seeds\r\n",
        "seed_value = 42 #@param {type:\"integer\"}\r\n",
        "\r\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "random.seed(seed_value)\r\n",
        "np.random.seed(seed_value)\r\n",
        "\r\n",
        "tf.compat.v1.set_random_seed(seed_value)\r\n",
        "\r\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\r\n",
        "tf.compat.v1.keras.backend.set_session(sess)\r\n",
        "\r\n",
        "# BERT params\r\n",
        "max_seq_length = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RncxAGjM4vm"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El70BLeMa4np"
      },
      "source": [
        "#@title df creation { form-width: \"25%\" }\n",
        "\n",
        "# the official dataset is identical to the provided one\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json -O training_set.json\n",
        "\n",
        "with open(\"training_set.json\", \"r\") as f:\n",
        "    json_file = json.load(f)\n",
        "data = json_file[\"data\"]\n",
        "\n",
        "rows = []\n",
        "for document in data:\n",
        "  for par in document['paragraphs']:\n",
        "    for qas in par['qas']:\n",
        "      rows.append({\n",
        "        'id' : qas['id'],\n",
        "        'title': document[\"title\"],\n",
        "        'passage': par['context'],\n",
        "        'question' : qas['question'],\n",
        "        'answer_idx' : (qas['answers'][0]['answer_start'], \n",
        "                    qas['answers'][0]['answer_start'] + len(qas['answers'][0]['text'])),\n",
        "        'answer_text' : qas['answers'][0]['text']\n",
        "      })\n",
        "\n",
        "df_original = pd.DataFrame(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHLX3Yv3ZBMB"
      },
      "source": [
        "#@title clean dataset { form-width: \"25%\" }\r\n",
        "\r\n",
        "!gcloud config set project feisty-mechanic-221914\r\n",
        "!gsutil cp gs://squad_squad/error_IDs.txt ./error_IDs.txt\r\n",
        "\r\n",
        "with open(\"error_IDs.txt\", \"r\") as f:\r\n",
        "    unwanted_id = f.read()\r\n",
        "\r\n",
        "unwanted_id = unwanted_id.split(\"\\n\")[:-1]\r\n",
        "df_bert = df_original.set_index('id')\r\n",
        "df_bert = df_bert.drop(unwanted_id)\r\n",
        "df_bert.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWs7lccpsj0Z"
      },
      "source": [
        "!pip install transformers\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUk5qzUOzr5e"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.is_fast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrFiLO78ieg-"
      },
      "source": [
        "def preprocess_bert(text):\n",
        "    #preprocesed_text = [\" \".join(str(line).split()) for line in text]\n",
        "    tokenized_text = tokenizer(text, return_offsets_mapping=True)\n",
        "\n",
        "    rows_out  = [{'input_ids': tokenized_text.input_ids[i],\n",
        "                  'offsets': tokenized_text.offset_mapping[i]} for i in range(len(text))]\n",
        "\n",
        "    return rows_out\n",
        "\n",
        "preprocess_bert([\"hi mum\", \"how are you\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORJXJrplu3be"
      },
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(10)):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX_ERg4QT6P_"
      },
      "source": [
        "#@title BERT preprocessing { form-width: \"25%\" }\r\n",
        "from tqdm import tqdm\r\n",
        "#vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\r\n",
        "#tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\r\n",
        "\r\n",
        "def preprocess_bert(text):\r\n",
        "    tokenized_text = tokenizer(list(text), return_offsets_mapping=True)\r\n",
        "\r\n",
        "    rows_out  = [{'input_ids': tokenized_text.input_ids[i],\r\n",
        "                  'offsets': tokenized_text.offset_mapping[i]} for i in range(len(text))]\r\n",
        "\r\n",
        "    return rows_out\r\n",
        "\r\n",
        "def labeling(df):\r\n",
        "    skip = []\r\n",
        "    ans_token_start = []\r\n",
        "    ans_token_end = []\r\n",
        "    input_word_ids = []\r\n",
        "    input_type_ids = []\r\n",
        "    input_mask = []\r\n",
        "    context_token_to_char = []\r\n",
        "\r\n",
        "    for id in tqdm(df.index):\r\n",
        "        answer = \" \".join(str(df.loc[id]['answer_text']).split())\r\n",
        "        tokenized_context = df.loc[id]['passage']\r\n",
        "        tokenized_question = df.loc[id]['question']\r\n",
        "\r\n",
        "        # mark all the character indexes in context that are also in answer     \r\n",
        "        is_char_in_ans = [0] * len(df_bert.loc[id]['passage'])\r\n",
        "        for idx in range(*df.loc[id]['answer_idx']):\r\n",
        "            is_char_in_ans[idx] = 1\r\n",
        "        ans_token_idx = []\r\n",
        "        # find all the tokens that are in the answers\r\n",
        "        for idx, (start, end) in enumerate(tokenized_context[\"offsets\"]): #start is index of the first character of the word, end is the index of the last character of the word\r\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\r\n",
        "                ans_token_idx.append(idx)\r\n",
        "        if len(ans_token_idx) == 0:\r\n",
        "            skip.append(id)\r\n",
        "            continue\r\n",
        "        # create inputs as usual\r\n",
        "        input_ids = tokenized_context['input_ids'] + tokenized_question['input_ids'][1:] #removing CLS from the beginning of the question \r\n",
        "        token_type_ids = [0] * len(tokenized_context['input_ids']) + [1] * len(tokenized_question['input_ids'][1:])\r\n",
        "        attention_mask = [1] * len(input_ids)\r\n",
        "        padding_length = max_seq_length - len(input_ids)\r\n",
        "        \r\n",
        "        # add padding if necessary\r\n",
        "        if padding_length > 0:\r\n",
        "            input_ids = input_ids + ([0] * padding_length)\r\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "        elif padding_length < 0:\r\n",
        "            skip.append(id)\r\n",
        "            continue\r\n",
        "        input_word_ids.append(np.array(input_ids))\r\n",
        "        input_type_ids.append(np.array(token_type_ids))\r\n",
        "        input_mask.append(np.array(attention_mask))\r\n",
        "        context_token_to_char.append(np.array(tokenized_context[\"offsets\"]))\r\n",
        "        ans_token_start.append(ans_token_idx[0])\r\n",
        "        ans_token_end.append(ans_token_idx[-1])\r\n",
        "\r\n",
        "    df = df.drop(skip)\r\n",
        "    df['input_word_ids'] = input_word_ids\r\n",
        "    df['input_type_ids'] = input_type_ids\r\n",
        "    df['input_mask'] = input_mask\r\n",
        "    df['context_token_to_char'] = context_token_to_char\r\n",
        "    df['ans_token_start'] = ans_token_start\r\n",
        "    df['ans_token_end'] = ans_token_end\r\n",
        "\r\n",
        "    return df    \r\n",
        "\r\n",
        "df_bert_preprocessed = df_bert.copy()\r\n",
        "# pre-process passage and question text\r\n",
        "print(\"Preprocessing passage...\")\r\n",
        "df_bert_preprocessed['passage'] = preprocess_bert(df_bert['passage'])\r\n",
        "print(\"Preprocessing question...\")\r\n",
        "df_bert_preprocessed['question'] = preprocess_bert(df_bert['question'])\r\n",
        "print(\"Building attention masks...\")\r\n",
        "df_bert_preprocessed = labeling(df_bert_preprocessed)\r\n",
        "df_bert_preprocessed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqUfNH08vPl2"
      },
      "source": [
        "    df_bert_preprocessed.to_pickle(\"df_bert_preprocessed_fast.pkl\")\n",
        "    !zip df_bert_preprocessed.pkl.zip df_bert_preprocessed_fast.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1su2V9XM93R"
      },
      "source": [
        "#Skip preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cKOshQJ64TS"
      },
      "source": [
        "#@title load/store { form-width: \"25%\" }\r\n",
        "import pickle\r\n",
        "load = True #@param {type: \"boolean\"}\r\n",
        "\r\n",
        "if load:\r\n",
        "    !gcloud config set project feisty-mechanic-221914\r\n",
        "    !gsutil cp gs://squad_squad/df_bert_preprocessed.pkl.zip ./df_bert_preprocessed.pkl.zip\r\n",
        "    !unzip -o ./df_bert_preprocessed.pkl.zip\r\n",
        "    df_bert_preprocessed = pd.read_pickle(\"./df_bert_preprocessed.pkl\")\r\n",
        "else:\r\n",
        "    df_bert_preprocessed.to_pickle(\"df_bert_preprocessed.pkl\")\r\n",
        "    !zip df_bert_preprocessed.pkl.zip df_bert_preprocessed.pkl\r\n",
        "\r\n",
        "    from google.colab import auth\r\n",
        "    auth.authenticate_user()\r\n",
        "    !gcloud config set project feisty-mechanic-221914\r\n",
        "    !gsutil cp ./df_bert_preprocessed.pkl.zip gs://squad_squad/df_bert_preprocessed.pkl.zip\r\n",
        "\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x101fcENCOs"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0PQiNubobIK"
      },
      "source": [
        "#@title split { form-width: \"25%\" }\r\n",
        "\r\n",
        "split_value = 0.1 #@param {type:\"number\"} \r\n",
        "val_dim = int(len(df_bert_preprocessed['title'].unique()) * split_value)\r\n",
        "val_titles = np.random.choice(df_bert_preprocessed['title'].unique(), size=val_dim, replace=False)\r\n",
        "\r\n",
        "df_bert_val = df_bert_preprocessed[df_bert_preprocessed['title'].isin(val_titles)]\r\n",
        "df_bert_train = df_bert_preprocessed[~(df_bert_preprocessed['title'].isin(val_titles))]\r\n",
        "\r\n",
        "x_train, y_train = [np.stack(df_bert_train[\"input_word_ids\"]),np.stack(df_bert_train[\"input_mask\"]),np.stack(df_bert_train[\"input_type_ids\"])],[np.stack(df_bert_train[\"ans_token_start\"]),np.stack(df_bert_train[\"ans_token_end\"])]\r\n",
        "x_eval, y_eval = [np.stack(df_bert_val[\"input_word_ids\"]),np.stack(df_bert_val[\"input_mask\"]),np.stack(df_bert_val[\"input_type_ids\"])],[np.stack(df_bert_val[\"ans_token_start\"]),np.stack(df_bert_val[\"ans_token_end\"])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg6vZ7MqqKU1"
      },
      "source": [
        "bert_hf_layer = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNc6G5936ucz"
      },
      "source": [
        "#@title model definition { form-width: \"25%\" }\r\n",
        "\r\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\r\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\r\n",
        "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\r\n",
        "\r\n",
        "#pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\r\n",
        "\r\n",
        "#HUGGINGFACE 🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗🤗\r\n",
        "sequence_output = bert_hf_layer([input_word_ids, \r\n",
        "                                 input_mask, \r\n",
        "                                 input_type_ids]).last_hidden_state\r\n",
        "\r\n",
        "#do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "\r\n",
        "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\r\n",
        "start_logits = layers.Flatten(name=\"flatten_start\")(start_logits)\r\n",
        "\r\n",
        "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\r\n",
        "end_logits = layers.Flatten(name=\"flatten_end\")(end_logits)\r\n",
        "\r\n",
        "start_probs = layers.Activation(keras.activations.softmax, name=\"softmax_start\")(start_logits)\r\n",
        "end_probs = layers.Activation(keras.activations.softmax, name=\"softmax_end\")(end_logits)\r\n",
        "\r\n",
        "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], \r\n",
        "                    outputs=[start_probs, end_probs],\r\n",
        "                    name=\"BERT_QA\")\r\n",
        "\r\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n",
        "\r\n",
        "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\r\n",
        "\r\n",
        "model.summary(line_length=150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nvbXaVJMW_A"
      },
      "source": [
        "#@title metrics { form-width: \"25%\" }\r\n",
        "\r\n",
        "def prec(y_true, y_pred):\r\n",
        "    sampled = tf.argmax(y_pred, axis=-1)\r\n",
        "    return 1 - tf.math.count_nonzero(tf.squeeze(tf.cast(y_true, tf.int64)) - sampled) / tf.cast(len(sampled), tf.int64)\r\n",
        "\r\n",
        "def dist(y_true, y_pred):\r\n",
        "    sampled = tf.argmax(y_pred, axis=-1)\r\n",
        "    return tf.reduce_sum(tf.abs(tf.squeeze(tf.cast(y_true, tf.int64)) - sampled)) / tf.cast(len(sampled), tf.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-Gml1Q-rBqr",
        "outputId": "d683399b-4904-4a19-8ce6-019d81209379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title train { form-width: \"25%\" }\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "batch_size = 8\r\n",
        "epochs = 200\r\n",
        "steps_per_epoch = 20\r\n",
        "saveDir = os.path.join(os.getcwd(), 'saved_models')\r\n",
        "if not os.path.isdir(saveDir):\r\n",
        "    os.makedirs(saveDir)\r\n",
        "chkpt = saveDir + '/squad_check.hdf5'\r\n",
        "\r\n",
        "ENABLE_WANDB = True        #@param {type:\"boolean\"}\r\n",
        "wandb_experiment_name = \"HF_BERT_FAST\"  #@param {type: \"string\"}\r\n",
        "if ENABLE_WANDB:\r\n",
        "    !pip install wandb > /dev/null\r\n",
        "    !wandb login wandb_api_token\r\n",
        "    import wandb\r\n",
        "    from wandb.keras import WandbCallback\r\n",
        "    wandb.init(project=\"SQUAD\", name=wandb_experiment_name)\r\n",
        "    wandb.config.batch_size = batch_size\r\n",
        "    wandb.config.epochs = epochs\r\n",
        "    \r\n",
        "es_cb = EarlyStopping(monitor='val_loss', patience=2,verbose=1, mode='auto')\r\n",
        "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, \r\n",
        "                        save_best_only=False, mode='auto', \r\n",
        "                        save_weights_only=True)\r\n",
        "\r\n",
        "callbacks = [es_cb, cp_cb]\r\n",
        "\r\n",
        "if ENABLE_WANDB:\r\n",
        "    callbacks.append(WandbCallback(log_batch_frequency=10,\r\n",
        "                                   save_weights_only=True))\r\n",
        "\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "model.compile(optimizer=optimizer, loss=[loss,loss], metrics=[prec,dist])\r\n",
        "history = model.fit(x_train, y_train, epochs=epochs,callbacks=callbacks, \r\n",
        "                    validation_data=(x_eval,y_eval),batch_size=batch_size)#,\r\n",
        "                    #steps_per_epoch = steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 3394/10014 [=========>....................] - ETA: 59:33 - loss: 3.6904 - softmax_start_loss: 1.8892 - softmax_end_loss: 1.8012 - softmax_start_prec: 0.5049 - softmax_start_dist: 12.5077 - softmax_end_prec: 0.5395 - softmax_end_dist: 12.7641"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XFVQKuuNEA8"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXAakwfMQYZa"
      },
      "source": [
        "#@title download best weights\r\n",
        "!wget https://wandb.ai/veri/SQUAD/runs/1t78812w/files/model-best.h5\r\n",
        "model.load_weights(\"model-best.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16MNlFdGQkPO"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "predictions = model.predict(x_eval)\r\n",
        "sampled_start = np.argmax(predictions[0], axis=-1)\r\n",
        "sampled_end = np.argmax(predictions[1], axis=-1)\r\n",
        "plt.figure(figsize=(30,30))\r\n",
        "plt.plot(y_eval[0], y_eval[1], \".\")\r\n",
        "plt.plot(sampled_start, sampled_end,\"*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GAiyOURzyC"
      },
      "source": [
        "!gsutil cp gs://squad_squad/evaluate.py ./evaluate.py\r\n",
        "!chmod +x ./evaluate.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLbE3Mcn0tBv"
      },
      "source": [
        "#@title BERT preprocessing { form-width: \"25%\" }\n",
        "from tqdm import tqdm\n",
        "#vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "#tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
        "\n",
        "\n",
        "\n",
        "df_bert_preprocessed = df_bert.copy()\n",
        "# pre-process passage and question text\n",
        "print(\"Preprocessing passage...\")\n",
        "df_bert_preprocessed['passage'] = preprocess_bert(df_bert['passage'])\n",
        "print(\"Preprocessing question...\")\n",
        "df_bert_preprocessed['question'] = preprocess_bert(df_bert['question'])\n",
        "print(\"Building attention masks...\")\n",
        "df_bert_preprocessed = labeling(df_bert_preprocessed)\n",
        "df_bert_preprocessed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dHYm-NCTmLB"
      },
      "source": [
        "# preprocess dev set\n",
        "\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -O test_set.json\n",
        "\n",
        "with open(\"test_set.json\", \"r\") as f:\n",
        "    json_file = json.load(f)\n",
        "data = json_file[\"data\"]\n",
        "\n",
        "rows = []\n",
        "for document in data:\n",
        "  for par in document['paragraphs']:\n",
        "    for qas in par['qas']:\n",
        "      rows.append({\n",
        "        'id' : qas['id'],\n",
        "        'title': document[\"title\"],\n",
        "        'passage': par['context'],\n",
        "        'question' : qas['question'],\n",
        "        'answer_idx' : (qas['answers'][0]['answer_start'], \n",
        "                    qas['answers'][0]['answer_start'] + len(qas['answers'][0]['text'])),\n",
        "        'answer_text' : qas['answers'][0]['text']\n",
        "      })\n",
        "\n",
        "df_dev = pd.DataFrame(rows)\n",
        "\n",
        "def preprocess_bert(text):\n",
        "    tokenized_text = tokenizer(list(text), return_offsets_mapping=True)\n",
        "\n",
        "    rows_out  = [{'input_ids': tokenized_text.input_ids[i],\n",
        "                  'offsets': tokenized_text.offset_mapping[i]} for i in range(len(text))]\n",
        "\n",
        "    return rows_out\n",
        "\n",
        "def labeling(df):\n",
        "    skip = []\n",
        "    ans_token_start = []\n",
        "    ans_token_end = []\n",
        "    input_word_ids = []\n",
        "    input_type_ids = []\n",
        "    input_mask = []\n",
        "    context_token_to_char = []\n",
        "\n",
        "    for id in tqdm(df.index):\n",
        "        answer = \" \".join(str(df.loc[id]['answer_text']).split())\n",
        "        tokenized_context = df.loc[id]['passage']\n",
        "        tokenized_question = df.loc[id]['question']\n",
        "\n",
        "        # mark all the character indexes in context that are also in answer     \n",
        "        is_char_in_ans = [0] * len(df_bert.loc[id]['passage'])\n",
        "        for idx in range(*df.loc[id]['answer_idx']):\n",
        "            is_char_in_ans[idx] = 1\n",
        "        ans_token_idx = []\n",
        "        # find all the tokens that are in the answers\n",
        "        for idx, (start, end) in enumerate(tokenized_context[\"offsets\"]): #start is index of the first character of the word, end is the index of the last character of the word\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "        if len(ans_token_idx) == 0:\n",
        "            skip.append(id)\n",
        "            continue\n",
        "        # create inputs as usual\n",
        "        input_ids = tokenized_context['input_ids'] + tokenized_question['input_ids'][1:] #removing CLS from the beginning of the question \n",
        "        token_type_ids = [0] * len(tokenized_context['input_ids']) + [1] * len(tokenized_question['input_ids'][1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        \n",
        "        # add padding if necessary\n",
        "        if padding_length > 0:\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:\n",
        "            skip.append(id)\n",
        "            continue\n",
        "        input_word_ids.append(np.array(input_ids))\n",
        "        input_type_ids.append(np.array(token_type_ids))\n",
        "        input_mask.append(np.array(attention_mask))\n",
        "        context_token_to_char.append(np.array(tokenized_context[\"offsets\"]))\n",
        "        ans_token_start.append(ans_token_idx[0])\n",
        "        ans_token_end.append(ans_token_idx[-1])\n",
        "\n",
        "    df = df.drop(skip)\n",
        "    df['input_word_ids'] = input_word_ids\n",
        "    df['input_type_ids'] = input_type_ids\n",
        "    df['input_mask'] = input_mask\n",
        "    df['context_token_to_char'] = context_token_to_char\n",
        "    df['ans_token_start'] = ans_token_start\n",
        "    df['ans_token_end'] = ans_token_end\n",
        "\n",
        "    return df, skip\n",
        "\n",
        "# pre-process passage and question text\n",
        "df_dev = df_dev.set_index('id')\n",
        "df_bert_dev = df_dev.copy()\n",
        "\n",
        "df_bert_dev['passage'] = preprocess_bert(df_dev['passage'])\n",
        "df_bert_dev['question'] = preprocess_bert(df_dev['question'])\n",
        "\n",
        "df_bert_dev, skipped = labeling(df_bert_dev)\n",
        "df_bert_dev.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmnMJuS8cNZU"
      },
      "source": [
        "x_test, y_test = [np.stack(df_bert_dev[\"input_word_ids\"]),np.stack(df_bert_dev[\"input_mask\"]),\n",
        "                  np.stack(df_bert_dev[\"input_type_ids\"])],[np.stack(df_bert_dev[\"ans_token_start\"]),\n",
        "                                                            np.stack(df_bert_dev[\"ans_token_end\"])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khj7lPPJeAP9"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
        "\n",
        "# def preprocess_bert(text):\n",
        "#     preprocessed_text = [\" \".join(str(line).split()) for line in text]\n",
        "#     tokenized_text = [tokenizer.encode(preprocessed_line) for preprocessed_line in preprocessed_text]\n",
        "#     return tokenized_text\n",
        "\n",
        "num_samples = len(df_bert_dev)\n",
        "predictions = model.predict(x_test)\n",
        "        \n",
        "start, end = list(np.argmax(predictions, axis=-1).squeeze())\n",
        "with open(\"dev_predictions.txt\",\"w\") as out:\n",
        "    out.write(\"{\")\n",
        "    for id in skipped:\n",
        "        out.write(f'''\"{id}\": \"42\",\\n''')\n",
        "\n",
        "    for ans_id in range(num_samples):\n",
        "        predicted_ans = tokenizer.decode(df_bert_dev.iloc[ans_id]['passage'].ids[start[ans_id] : end[ans_id]+1])\n",
        "        if ans_id == num_samples-1:\n",
        "            out.write(f'''\"{df_bert_dev.index[ans_id]}\": \"{predicted_ans.replace('\"',\"\")}\"''')\n",
        "        else:\n",
        "            out.write(f'''\"{df_bert_dev.index[ans_id]}\": \"{predicted_ans.replace('\"',\"\")}\",\\n''')\n",
        "        #print(tokenizer.decode(df_bert_dev.iloc[ans_id]['question'].ids), predicted_ans, df_bert_dev.index[ans_id])\n",
        "\n",
        "    out.write(\"}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74EEAaH1LNK2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9COVdfagco91"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#predictions = model.predict(x_test)\n",
        "sampled_start = np.argmax(predictions[0], axis=-1)\n",
        "sampled_end = np.argmax(predictions[1], axis=-1)\n",
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.plot(y_test[0], y_test[1], \".\")\n",
        "plt.plot(sampled_start, sampled_end,\"*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGh_peiPTCOE"
      },
      "source": [
        "!python3 evaluate.py test_set.json dev_predictions.txt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGh7mkarf_a1"
      },
      "source": [
        "# inference with custom context and question\r\n",
        "\r\n",
        "def custom_inference(context, question):\r\n",
        "    preprocessed_context = \" \".join(str(context).split())\r\n",
        "    preprocessed_question = \" \".join(str(question).split())\r\n",
        "    tokenized_context = tokenizer(preprocessed_context)\r\n",
        "    tokenized_question = tokenizer(preprocessed_question)\r\n",
        "    input_ids = tokenized_context[\"input_ids\"] + tokenized_question[\"input_ids\"][1:]\r\n",
        "    token_type_ids = [0] * len(tokenized_context[\"input_ids\"]) + [1] * len(tokenized_question[\"input_ids\"][1:])\r\n",
        "    attention_mask = [1] * len(input_ids)\r\n",
        "    padding_length = max_seq_length - len(input_ids)\r\n",
        "    if padding_length > 0:\r\n",
        "        input_ids = input_ids + ([0] * padding_length)\r\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "    else:\r\n",
        "        print(\"Error! The input is too long\")\r\n",
        "    input_word_ids = np.array(input_ids)\r\n",
        "    input_mask = np.array(attention_mask)\r\n",
        "    input_type_ids = np.array(token_type_ids)\r\n",
        "    x = [np.expand_dims(input_word_ids, axis =0), np.expand_dims(input_mask, axis = 0), \r\n",
        "         np.expand_dims(input_type_ids,axis=0)]\r\n",
        "    predictions = model.predict(x)\r\n",
        "    start, end = list(np.argmax(predictions, axis=-1).squeeze())\r\n",
        "    predicted_ans = tokenizer.decode(tokenized_context[\"input_ids\"][start : end+1])\r\n",
        "    return predicted_ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0ZGxDrYtnJL"
      },
      "source": [
        "context = \"Thomas Cruise Mapother IV (born July 3, 1962) is an American actor and producer. He has received various accolades for his work, including three Golden Globe Awards and three nominations for Academy Awards. He is one of the highest-paid actors in the world. His films have grossed over $4 billion in North America and over $10.1 billion worldwide, making him one of the highest-grossing box office stars of all time. Cruise began acting in the early 1980s and made his breakthrough with leading roles in the comedy film Risky Business (1983) and action drama film Top Gun (1986). Critical acclaim came with his roles in the drama films The Color of Money (1986), Rain Man (1988), and Born on the Fourth of July (1989). For his portrayal of Ron Kovic in the latter, he won a Golden Globe Award and received a nomination for the Academy Award for Best Actor. As a leading Hollywood star in the 1990s, he starred in several commercially successful films, including the drama A Few Good Men (1992), the thriller The Firm (1993), the horror film Interview with the Vampire (1994), and the romance Jerry Maguire (1996). For his role in the latter, he won a Golden Globe Award for Best Actor and received his second Academy Award nomination.\"\r\n",
        "question = \"What was the first film Tom Cruise acted in?\"\r\n",
        "\r\n",
        "\r\n",
        "predicted_answer = custom_inference(context, question)\r\n",
        "print(predicted_answer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}