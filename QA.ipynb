{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-SQuAD-squad/IR-QA/blob/regression/QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmkNAxSnaD9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "00db9742-63fc-4976-a200-17dc7ba556b5"
      },
      "source": [
        "#@title Init { form-width: \"25%\" }\r\n",
        "import os\r\n",
        "import random\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "pd.set_option('display.max_colwidth', -1)\r\n",
        "\r\n",
        "# fix random seeds\r\n",
        "seed_value = 42 #@param {type:\"integer\"}\r\n",
        "\r\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "random.seed(seed_value)\r\n",
        "np.random.seed(seed_value)\r\n",
        "\r\n",
        "tf.compat.v1.set_random_seed(seed_value)\r\n",
        "\r\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\r\n",
        "tf.compat.v1.keras.backend.set_session(sess)\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKJ2cwO36_RS"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El70BLeMa4np",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0e5387c4-65a9-40cf-c7e6-e78037eccd7c"
      },
      "source": [
        "#@title df creation { form-width: \"25%\" }\n",
        "\n",
        "# the official dataset is identical to the provided one\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json -O training_set.json\n",
        "\n",
        "with open(\"training_set.json\", \"r\") as f:\n",
        "    json_file = json.load(f)\n",
        "data = json_file[\"data\"]\n",
        "\n",
        "rows = []\n",
        "for document in data:\n",
        "  for par in document['paragraphs']:\n",
        "    for qas in par['qas']:\n",
        "      rows.append({\n",
        "        'id' : qas['id'],\n",
        "        'title': document[\"title\"],\n",
        "        'passage': par['context'],\n",
        "        'question' : qas['question'],\n",
        "        'answer_idx' : (qas['answers'][0]['answer_start'], \n",
        "                    qas['answers'][0]['answer_start'] + len(qas['answers'][0]['text'])),\n",
        "        'answer_text' : qas['answers'][0]['text']\n",
        "      })\n",
        "\n",
        "df_original = pd.DataFrame(rows)\n",
        "df_original.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-12 16:27:05--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.110.153, 185.199.109.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [application/json]\n",
            "Saving to: ‘training_set.json’\n",
            "\n",
            "training_set.json   100%[===================>]  28.88M   150MB/s    in 0.2s    \n",
            "\n",
            "2021-02-12 16:27:05 (150 MB/s) - ‘training_set.json’ saved [30288272/30288272]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_idx</th>\n",
              "      <th>answer_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</td>\n",
              "      <td>(515, 541)</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>(188, 213)</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame is beside to which structure?</td>\n",
              "      <td>(279, 296)</td>\n",
              "      <td>the Main Building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>(381, 420)</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</td>\n",
              "      <td>What sits on top of the Main Building at Notre Dame?</td>\n",
              "      <td>(92, 126)</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id  ...                              answer_text\n",
              "0  5733be284776f41900661182  ...  Saint Bernadette Soubirous             \n",
              "1  5733be284776f4190066117f  ...  a copper statue of Christ              \n",
              "2  5733be284776f41900661180  ...  the Main Building                      \n",
              "3  5733be284776f41900661181  ...  a Marian place of prayer and reflection\n",
              "4  5733be284776f4190066117e  ...  a golden statue of the Virgin Mary     \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtNxXkwi7lEH"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feNDF6x2Qeb1",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b421ec6-f39e-4325-e329-a40347f57782"
      },
      "source": [
        "#@title preprocessing { form-width: \"25%\" }\r\n",
        "\r\n",
        "import nltk\r\n",
        "import re \r\n",
        "import math\r\n",
        "import random as rand\r\n",
        "\r\n",
        "def preprocess_text(text):\r\n",
        "    \"\"\"\r\n",
        "    Given an iterable containing sentences, pre-process each sentence.\r\n",
        "\r\n",
        "    :param: \r\n",
        "        - text: list of text to be pre-processed (Iterable)\r\n",
        "    :return:\r\n",
        "        - text: pre-processed text (List)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    REPLACE_WITH_SPACE = re.compile(r\"\\n\") \r\n",
        "    text = [REPLACE_WITH_SPACE.sub(\" \", line) for line in text]\r\n",
        "\r\n",
        "    # we don't remove symbols, but just put a space before and after them. We did this because we noticed that Glove contains an embedding also for\r\n",
        "    # them, so, in this way, we are able to split these symbols from the text when computing sentence tokens\r\n",
        "    text = [re.sub(r\"([(.;:!\\'ˈ~?,\\\"(\\[\\])\\\\\\/\\-–\\t```<>_#$€@%*+—°′″“”×’^₤₹‘])\", r' \\1 ', line) for line in text]\r\n",
        "\r\n",
        "    # we noticed that in the text sometimes we find numbers and the following word merged together (ex: 1980february),\r\n",
        "    # so we put a space between the number and the word\r\n",
        "    text = [re.sub(r\"(\\d+)([a-z]+)\", r'\\1 \\2', line) for line in text] \r\n",
        "    text = [re.sub('\\s{2,}', ' ', line.strip()) for line in text]   # replacing more than one consecutive blank spaces with only one of them\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "# Creating a copy of the original dataframe (we do this because we want to be able to compare the results of our processing with the original data)\r\n",
        "df = df_original.copy()\r\n",
        "\r\n",
        "# pre-process passage and question text\r\n",
        "df['passage'] = preprocess_text(df_original['passage'])\r\n",
        "df['question'] = preprocess_text(df_original['question'])\r\n",
        "df['answer_text'] = preprocess_text(df_original['answer_text'])\r\n",
        "\r\n",
        "# Comparing Original and Pre-Processed\r\n",
        "for i in range(3):\r\n",
        "    a = rand.randint(0,1000)\r\n",
        "    print('ORIGINAL AND PREPROCESSED PASSAGE:')\r\n",
        "    print(df_original.iloc[a]['passage'])\r\n",
        "    print(df.iloc[a]['passage'])\r\n",
        "    \r\n",
        "    print()\r\n",
        "    print('ORIGINAL AND PREPROCESSED QUESTION:')\r\n",
        "    print(df_original.iloc[a]['question'])\r\n",
        "    print(df.iloc[a]['question'])\r\n",
        "    print()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORIGINAL AND PREPROCESSED PASSAGE:\n",
            "In 2015 Beyoncé signed an open letter which the ONE Campaign had been collecting signatures for; the letter was addressed to Angela Merkel and Nkosazana Dlamini-Zuma, urging them to focus on women as they serve as the head of the G7 in Germany and the AU in South Africa respectively, which will start to set the priorities in development funding before a main UN summit in September 2015 that will establish new development goals for the generation.\n",
            "In 2015 Beyoncé signed an open letter which the ONE Campaign had been collecting signatures for ; the letter was addressed to Angela Merkel and Nkosazana Dlamini - Zuma , urging them to focus on women as they serve as the head of the G7 in Germany and the AU in South Africa respectively , which will start to set the priorities in development funding before a main UN summit in September 2015 that will establish new development goals for the generation .\n",
            "\n",
            "ORIGINAL AND PREPROCESSED QUESTION:\n",
            "An important UN summit took place when?\n",
            "An important UN summit took place when ?\n",
            "\n",
            "ORIGINAL AND PREPROCESSED PASSAGE:\n",
            "The Rev. John J. Cavanaugh, C.S.C. served as president from 1946 to 1952. Cavanaugh's legacy at Notre Dame in the post-war years was devoted to raising academic standards and reshaping the university administration to suit it to an enlarged educational mission and an expanded student body and stressing advanced studies and research at a time when Notre Dame quadrupled in student census, undergraduate enrollment increased by more than half, and graduate student enrollment grew fivefold. Cavanaugh also established the Lobund Institute for Animal Studies and Notre Dame's Medieval Institute. Cavanaugh also presided over the construction of the Nieuwland Science Hall, Fisher Hall, and the Morris Inn, as well as the Hall of Liberal Arts (now O'Shaughnessy Hall), made possible by a donation from I.A. O'Shaughnessy, at the time the largest ever made to an American Catholic university. Cavanaugh also established a system of advisory councils at the university, which continue today and are vital to the university's governance and development\n",
            "The Rev . John J . Cavanaugh , C . S . C . served as president from 1946 to 1952 . Cavanaugh ' s legacy at Notre Dame in the post - war years was devoted to raising academic standards and reshaping the university administration to suit it to an enlarged educational mission and an expanded student body and stressing advanced studies and research at a time when Notre Dame quadrupled in student census , undergraduate enrollment increased by more than half , and graduate student enrollment grew fivefold . Cavanaugh also established the Lobund Institute for Animal Studies and Notre Dame ' s Medieval Institute . Cavanaugh also presided over the construction of the Nieuwland Science Hall , Fisher Hall , and the Morris Inn , as well as the Hall of Liberal Arts ( now O ' Shaughnessy Hall ) , made possible by a donation from I . A . O ' Shaughnessy , at the time the largest ever made to an American Catholic university . Cavanaugh also established a system of advisory councils at the university , which continue today and are vital to the university ' s governance and development\n",
            "\n",
            "ORIGINAL AND PREPROCESSED QUESTION:\n",
            "Which institute involving animal life did Cavanaugh create at Notre Dame?\n",
            "Which institute involving animal life did Cavanaugh create at Notre Dame ?\n",
            "\n",
            "ORIGINAL AND PREPROCESSED PASSAGE:\n",
            "The university first offered graduate degrees, in the form of a Master of Arts (MA), in the 1854–1855 academic year. The program expanded to include Master of Laws (LL.M.) and Master of Civil Engineering in its early stages of growth, before a formal graduate school education was developed with a thesis not required to receive the degrees. This changed in 1924 with formal requirements developed for graduate degrees, including offering Doctorate (PhD) degrees. Today each of the five colleges offer graduate education. Most of the departments from the College of Arts and Letters offer PhD programs, while a professional Master of Divinity (M.Div.) program also exists. All of the departments in the College of Science offer PhD programs, except for the Department of Pre-Professional Studies. The School of Architecture offers a Master of Architecture, while each of the departments of the College of Engineering offer PhD programs. The College of Business offers multiple professional programs including MBA and Master of Science in Accountancy programs. It also operates facilities in Chicago and Cincinnati for its executive MBA program. Additionally, the Alliance for Catholic Education program offers a Master of Education program where students study at the university during the summer and teach in Catholic elementary schools, middle schools, and high schools across the Southern United States for two school years.\n",
            "The university first offered graduate degrees , in the form of a Master of Arts ( MA ) , in the 1854 – 1855 academic year . The program expanded to include Master of Laws ( LL . M . ) and Master of Civil Engineering in its early stages of growth , before a formal graduate school education was developed with a thesis not required to receive the degrees . This changed in 1924 with formal requirements developed for graduate degrees , including offering Doctorate ( PhD ) degrees . Today each of the five colleges offer graduate education . Most of the departments from the College of Arts and Letters offer PhD programs , while a professional Master of Divinity ( M . Div . ) program also exists . All of the departments in the College of Science offer PhD programs , except for the Department of Pre - Professional Studies . The School of Architecture offers a Master of Architecture , while each of the departments of the College of Engineering offer PhD programs . The College of Business offers multiple professional programs including MBA and Master of Science in Accountancy programs . It also operates facilities in Chicago and Cincinnati for its executive MBA program . Additionally , the Alliance for Catholic Education program offers a Master of Education program where students study at the university during the summer and teach in Catholic elementary schools , middle schools , and high schools across the Southern United States for two school years .\n",
            "\n",
            "ORIGINAL AND PREPROCESSED QUESTION:\n",
            "What type of degree is an M.Div.?\n",
            "What type of degree is an M . Div . ?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXg4fox1SVVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "75303beb-b59a-4b10-a868-7ac3a588f4f4"
      },
      "source": [
        "#@title answer word idx + error catching { form-width: \"25%\" }\r\n",
        "\r\n",
        "unwanted_rows = set()   # this set will contain the indices of rows containing errors (thus we will remove these rows from the dataframe)\r\n",
        "unwanted_id = set()    # this set will contain the IDs of rows containing errors \r\n",
        "word_idx = []   # will contain the start and end indices of each answer in the corresponing passage\r\n",
        "impossible_count = 0\r\n",
        "\r\n",
        "for i in df_original.index:\r\n",
        "\r\n",
        "    # extracting one answer and the corresponding passage\r\n",
        "    answer = np.array(df[\"answer_text\"][i].split())\r\n",
        "    passage = np.array(df[\"passage\"][i].split())\r\n",
        "\r\n",
        "    l = len(answer)\r\n",
        "    idx = []    # this list will contain the start and end indices of the answer occurrence in the corresponding passage (these could be more than 1! ex: we find the answer \"rome\" in 2 distinct positions in the same passage)\r\n",
        "    counts = []     # counts will contain how many characters there are before the answer start in the passage\r\n",
        "    char_count = 0\r\n",
        "    for j in range(len(passage)-l+1):   # for each token in the passage, check if the answer starts from that token\r\n",
        "        if (answer == passage[j:j+l]).all():\r\n",
        "            idx.append((j, j+l))    # j is the start_index and j+l is the end_index of the answer in the passage\r\n",
        "            counts.append(char_count)    # char_count is the count of characters before the start of the answer\r\n",
        "        char_count += len(passage[j])\r\n",
        "    if len(counts) == 0:    # no answer found in the passage. Probably there is an error in the dataset (for instance the answer is \"7\", but in the text it is written like \"seven\")\r\n",
        "        unwanted_rows.add(i)\r\n",
        "        unwanted_id.add(df_original[\"id\"][i])\r\n",
        "        word_idx.append((-1, -1))   # stating that there was an error\r\n",
        "        impossible_count+=1\r\n",
        "\r\n",
        "        ## printing original question, answer and passage (by using the answer id to find the right dataframe row)\r\n",
        "        #print(str(df_original[\"question\"][df_original[\"id\"] == df[\"id\"][i]]))\r\n",
        "        #print(str(df_original['passage'][df_original['id'] == df['id'][i]]))\r\n",
        "        #print(str(df_original['answer_text'][df_original['id'] == df['id'][i]]))\r\n",
        "        #print(\"answer extracted from: {0}\".format((df_original['passage'][i][df_original[\"answer_idx\"][i][0]-3:df_original[\"answer_idx\"][i][1]+3])))\r\n",
        "        #print()\r\n",
        "\r\n",
        "    else:   # answer found in the passage\r\n",
        "        # if more than one answer correspondence was found in the passage, we take the one whose start index is nearer the start index given in the dataset \r\n",
        "        # (these 2 starting indices do not match perfectly because we are working on the preoprocessed text, so our starting index is a little bit different from the original)\r\n",
        "        n_spaces_original = df_original[\"passage\"][i][:df_original[\"answer_idx\"][i][0]].count(\" \")    # counting how many spaces there are in the original passage before the answer\r\n",
        "        n_newline_original = df_original[\"passage\"][i][:df_original[\"answer_idx\"][i][0]].count(\"\\n\")    # counting how many newline characters there are in the original passage before the answer\r\n",
        "        s = np.abs(np.array(counts)-(df[\"answer_idx\"][i][0]-n_spaces_original-n_newline_original))\r\n",
        "\r\n",
        "        if (0 not in s) and len(s)>1:   # in this case the answer was found in the passage, but the start index specified in the dataset is not the right one (it indicates a wrong occurrence of the answer)\r\n",
        "            unwanted_rows.add(i)\r\n",
        "            unwanted_id.add(df_original[\"id\"][i])\r\n",
        "            word_idx.append((-1, -1))   # stating that there was an error\r\n",
        "            impossible_count+=1\r\n",
        "\r\n",
        "            #print(str(df_original[\"question\"][df_original[\"id\"]==df[\"id\"][i]]))\r\n",
        "            #print(str(df_original['passage'][df_original['id'] == df['id'][i]]))\r\n",
        "            #print(str(df_original['answer_text'][df_original['id'] == df['id'][i]]))\r\n",
        "            #print(\"answer extracted from: {0}\".format((df_original['passage'][i][df_original[\"answer_idx\"][i][0]-3:df_original[\"answer_idx\"][i][1]+3])))\r\n",
        "            #print()\r\n",
        "        else:\r\n",
        "            word_idx.append(idx[np.argmin(s)])\r\n",
        "            # print(df_original[\"answer_text\"][i])\r\n",
        "            # print(passage[idx[np.argmin(s)][0]:idx[np.argmin(s)][1]])\r\n",
        "\r\n",
        "print(\"The number of rows that we will remove from the dataframe (because contain errors) are {0}\".format(impossible_count))\r\n",
        "\r\n",
        "# adding a new column to the dataframe containing the word indices of the answer in the splitted passage\r\n",
        "df[\"word_idx_answer\"] = word_idx\r\n",
        "df[\"passage\"]=df[\"passage\"].str.lower()\r\n",
        "df[\"question\"]=df[\"question\"].str.lower()\r\n",
        "df[\"answer_text\"]=df[\"answer_text\"].str.lower()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of rows that we will remove from the dataframe (because contain errors) are 233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL9cFDAeTdT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "43c4c80a-6a6e-4eb4-9d0a-e92b9d17f387"
      },
      "source": [
        "#@title build voc { form-width: \"25%\" }\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def build_vocabulary(text):\r\n",
        "    \"\"\"\r\n",
        "    Given a list of words, builds the corresponding word vocabulary and the mappings from words to indices and vice-versa.\r\n",
        "\r\n",
        "    :param: \r\n",
        "        - text: list of words from which we want to build the vocabularies (List)\r\n",
        "    :return:\r\n",
        "        - idx2word: index to word mapping (Dict)\r\n",
        "        - word2idx: word to index mapping (Dict)\r\n",
        "        - set_vocab: set of unique terms that build up the vocabulary\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Creating a set to eliminate repeated words\r\n",
        "    set_vocab = ['<PAD>']+sorted(set(text)) # here we add the padding token as the first element of the set\r\n",
        "\r\n",
        "    # Creating a mapping from unique words to indices\r\n",
        "    word2idx = {u:i for i, u in enumerate(set_vocab)}   # the padding token will have 0 index\r\n",
        "    # Creating a mapping from indices to unique words\r\n",
        "    idx2word = {i:u for i, u in enumerate(set_vocab)}\r\n",
        "\r\n",
        "    return idx2word,word2idx,set_vocab\r\n",
        "\r\n",
        "# Creating a list containing all the passage and question text splitted in words\r\n",
        "text =  ' '.join(np.concatenate((df['passage'],df['question']))).split(' ')\r\n",
        "# Displaying first 100 words\r\n",
        "print(text[:100])\r\n",
        "# calling the build_vocabulary function to obtain the vocab and the mappings\r\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(text)\r\n",
        "\r\n",
        "print('[Debug] Index -> Word vocabulary size: {}'.format(len(idx_to_word)))\r\n",
        "print('[Debug] Word -> Index vocabulary size: {}'.format(len(word_to_idx)))\r\n",
        "print('[Debug] Some words: {}'.format([(idx_to_word[idx], idx) for idx in np.arange(100)]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '\"', 'venite', 'ad', 'me', 'omnes', '\"', '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary']\n",
            "[Debug] Index -> Word vocabulary size: 82630\n",
            "[Debug] Word -> Index vocabulary size: 82630\n",
            "[Debug] Some words: [('<PAD>', 0), ('!', 1), ('\"', 2), ('#', 3), ('$', 4), ('%', 5), ('&', 6), (\"'\", 7), ('(', 8), (')', 9), ('*', 10), ('+', 11), (',', 12), ('-', 13), ('.', 14), ('/', 15), ('0', 16), ('00', 17), ('000', 18), ('0000', 19), ('0000222556', 20), ('000a', 21), ('000rmb', 22), ('001', 23), ('0018', 24), ('002', 25), ('0028', 26), ('0029670', 27), ('003', 28), ('004', 29), ('0042', 30), ('0043', 31), ('005', 32), ('0054', 33), ('006', 34), ('0065', 35), ('007', 36), ('0071', 37), ('0079', 38), ('00794', 39), ('008', 40), ('00b7', 41), ('00e9', 42), ('01', 43), ('010', 44), ('011', 45), ('012', 46), ('012f', 47), ('013', 48), ('014', 49), ('0147', 50), ('015', 51), ('016', 52), ('017', 53), ('018', 54), ('019', 55), ('02', 56), ('020', 57), ('0208', 58), ('021', 59), ('022', 60), ('024', 61), ('0243', 62), ('025', 63), ('026', 64), ('027', 65), ('029', 66), ('03', 67), ('030', 68), ('0301', 69), ('0307', 70), ('031', 71), ('032', 72), ('033', 73), ('034', 74), ('035', 75), ('0358', 76), ('036', 77), ('037', 78), ('039', 79), ('04', 80), ('040', 81), ('042', 82), ('043', 83), ('044', 84), ('045', 85), ('046', 86), ('047', 87), ('048', 88), ('049', 89), ('05', 90), ('050', 91), ('052', 92), ('053', 93), ('055', 94), ('056', 95), ('057', 96), ('058', 97), ('059', 98), ('05946', 99)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8xiVS-hXc4g",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37cfd4f-e210-4cde-b187-499f5bbd7c4f"
      },
      "source": [
        "#@title create embedding matrix { form-width: \"25%\" }\r\n",
        "\r\n",
        "import gensim\r\n",
        "import gensim.downloader as gloader\r\n",
        "\r\n",
        "def load_embedding_model(model_type, embedding_dimension=50):\r\n",
        "    \"\"\"\r\n",
        "    Loads a pre-trained word embedding model via gensim library.\r\n",
        "\r\n",
        "    :params:\r\n",
        "        - model_type: name of the word embedding model to load.\r\n",
        "        - embedding_dimension: size of the embedding space to consider\r\n",
        "\r\n",
        "    :return:\r\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    download_path = \"\"\r\n",
        "\r\n",
        "    # Find the correct embedding model name\r\n",
        "    if model_type.strip().lower() == 'word2vec':\r\n",
        "        download_path = \"word2vec-google-news-300\"\r\n",
        "\r\n",
        "    elif model_type.strip().lower() == 'glove':\r\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\r\n",
        "\r\n",
        "    else:\r\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\r\n",
        "\r\n",
        "    # Check download\r\n",
        "    try:\r\n",
        "        emb_model = gloader.load(download_path)\r\n",
        "    except ValueError as e:\r\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\r\n",
        "        print(\"Word2Vec: 300\")\r\n",
        "        print(\"Glove: 50, 100, 200, 300\")\r\n",
        "        raise e\r\n",
        "\r\n",
        "    return emb_model\r\n",
        "\r\n",
        "\r\n",
        "def check_OOV_terms(embedding_model, word_listing):\r\n",
        "    \"\"\"\r\n",
        "    Checks differences between pre-trained embedding model vocabulary\r\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\r\n",
        "\r\n",
        "    :params:\r\n",
        "        - embedding_model: pre-trained word embedding model (gensim wrapper)\r\n",
        "        - word_listing: dataset specific vocabulary (list)\r\n",
        "\r\n",
        "    :return:\r\n",
        "        - list of OOV terms\r\n",
        "    \"\"\"\r\n",
        "    # Creating a list for the OOV words\r\n",
        "    oov = []\r\n",
        "    for word in word_listing:\r\n",
        "        # Checking if the word is in the embedding_model\r\n",
        "        if word not in embedding_model:\r\n",
        "            oov.append(word)\r\n",
        "    return oov\r\n",
        "\r\n",
        "\r\n",
        "def build_embedding_matrix_w_random(embedding_model, embedding_dimension, word_to_idx, oov_terms):\r\n",
        "    \"\"\"\r\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\r\n",
        "\r\n",
        "    :params: \r\n",
        "        - embedding_model: pre-trained word embedding model (gensim wrapper)\r\n",
        "        - word_to_idx: vocabulary map (word -> index) (dict)\r\n",
        "        - oov_terms: list of OOV terms (list)\r\n",
        "\r\n",
        "    :return\r\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\r\n",
        "    \"\"\"\r\n",
        "    embedding_matrix = []\r\n",
        "    for word in word_to_idx:\r\n",
        "        if word in oov_terms:\r\n",
        "            embedding_matrix.append(np.random.rand(embedding_dimension))\r\n",
        "        else:\r\n",
        "             embedding_matrix.append(embedding_model[word])\r\n",
        "    return np.array(embedding_matrix)\r\n",
        "\r\n",
        "# we used Glove with embedding dimension 100 for our final tests\r\n",
        "embedding_model_type = \"Glove\"\r\n",
        "embedding_dimension = 100\r\n",
        "embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)\r\n",
        "\r\n",
        "# checking how many OOV terms we have\r\n",
        "oov_terms = check_OOV_terms(embedding_model, word_listing)\r\n",
        "\r\n",
        "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), len(oov_terms)/len(word_listing)*100))\r\n",
        "\r\n",
        "embedding_matrix = build_embedding_matrix_w_random(embedding_model, embedding_dimension, word_to_idx, oov_terms)\r\n",
        "\r\n",
        "print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\r\n",
        "\r\n",
        "print(\"Some OOV terms: \", oov_terms[:100])   # this was useful to understand if we could improve pre-processing"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Total OOV terms: 15985 (19.35%)\n",
            "Embedding matrix shape: (82630, 100)\n",
            "Some OOV terms:  ['<PAD>', '0000222556', '000a', '000rmb', '0018', '0028', '0029670', '0042', '0043', '0054', '0065', '0071', '0079', '00794', '00b7', '00e9', '012f', '0243', '0307', '0358', '036', '042', '057', '058', '05946', '062', '064', '066', '067', '069', '073', '078', '079', '083', '085', '088', '096', '097', '0fm', '100l', '10217', '10925', '10ffff', '10z', '10¢', '10−10', '10−12', '10−19', '10−2', '10−3', '10−34', '10−6', '10−8', '10−9', '1101010', '11092', '11114', '11172', '11246', '12232', '12291', '122ad', '1234567', '12750', '130h', '130j', '131\\u202f000', '133cs', '13500', '13526', '13818', '145a', '146a', '14fdr', '14−17', '15408', '16041', '16384', '177847', '178b', '17½', '17\\u202f000', '180ad', '18578', '1881−82', '1911a1', '1967a', '1967b', '197bc', '19n', '19sc111', '19˚n', '19˚s', '1fm', '1n4148', '1q08', '1\\u202f800', '1⁄10', '1⁄14', '1⁄2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmT-17Ydn905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "29ae7080-9218-4946-eeed-bbc89e01ebbf"
      },
      "source": [
        "#@title dataset cleaning { form-width: \"25%\" }\r\n",
        "\r\n",
        "# filters out question shorter than 10 characters\r\n",
        "for i,question in enumerate(df['question']):\r\n",
        "    if len(question)<=10:\r\n",
        "        #print(df_original.iloc[i]) \r\n",
        "        #print(df_original['passage'][i])\r\n",
        "        #print()\r\n",
        "        unwanted_rows.add(i)\r\n",
        "        unwanted_id.add(df[\"id\"][i])\r\n",
        "\r\n",
        "# ERROR CHECK before removing rows\r\n",
        "print(\"Number of errors found: {0}\\n\".format(str(len(unwanted_id))))\r\n",
        "some_error_id = random.sample(unwanted_id, 5)\r\n",
        "for id in some_error_id:\r\n",
        "    row_with_error = df_original[df_original[\"id\"] == id].to_dict(\"list\")\r\n",
        "    print(\"Question: {0}\".format(row_with_error[\"question\"][0]))\r\n",
        "    print(\"Passage: {0}\".format(row_with_error[\"passage\"][0]))\r\n",
        "    print(\"Answer: {0}\".format(row_with_error[\"answer_text\"][0]))\r\n",
        "    print(\"Answer extracted from: {0}\".format(row_with_error[\"passage\"][0][row_with_error[\"answer_idx\"][0][0]-3:row_with_error[\"answer_idx\"][0][1]+3]))\r\n",
        "    print()\r\n",
        "\r\n",
        "# Creating a txt file containing IDs of rows with errors for tutors\r\n",
        "with open(\"error_IDs.txt\", \"a\") as f:\r\n",
        "    for error_id in unwanted_id:\r\n",
        "        f.write(error_id + \"\\n\")\r\n",
        "\r\n",
        "df_clean = df.drop(list(unwanted_rows))\r\n",
        "df_clean = df_clean.reset_index()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of errors found: 235\n",
            "\n",
            "Question: The Jataka tales of the Theravada happened in what century?\n",
            "Passage: This narrative draws on the Nidānakathā of the Jataka tales of the Theravada, which is ascribed to Buddhaghoṣa in the 5th century CE. Earlier biographies such as the Buddhacarita, the Lokottaravādin Mahāvastu, and the Sarvāstivādin Lalitavistara Sūtra, give different accounts. Scholars are hesitant to make unqualified claims about the historical facts of the Buddha's life. Most accept that he lived, taught and founded a monastic order, but do not consistently accept all of the details contained in his biographies.\n",
            "Answer: 5th ce\n",
            "Answer extracted from: he 5th centu\n",
            "\n",
            "Question: How many commercial airports are under construction in Saint Helena?\n",
            "Passage: Saint Helena is one of the most remote islands in the world, has one commercial airport under construction, and travel to the island is by ship only. A large military airfield is located on Ascension Island, with two Friday flights to RAF Brize Norton, England (as from September 2010). These RAF flights offer a limited number of seats to civilians.\n",
            "Answer: 1\n",
            "Answer extracted from:  2010).\n",
            "\n",
            "Question: Did the patients with natural scenes have longer postoperative hospital stays?\n",
            "Passage: A study conducted in 1972 and 1981, documented by Robert Ulrich, surveyed 23 surgical patients assigned to rooms looking out on a natural scene. The study concluded that patients assigned to rooms with windows allowing lots of natural light had shorter postoperative hospital stays, received fewer negative evaluative comments in nurses’ notes, and took fewer potent analegesics than 23 matched patients in similar rooms with windows facing a brick wall. This study suggests that due to the nature of the scenery and daylight exposure was indeed healthier for patients as opposed to those exposed to little light from the brick wall. In addition to increased work performance, proper usage of windows and daylighting crosses the boundaries between pure aesthetics and overall health.\n",
            "Answer: no\n",
            "Answer extracted from: s’ notes\n",
            "\n",
            "Question: Who was Claude R Kirk \n",
            "Passage: The first post-Reconstruction era Republican elected to Congress from Florida was William C. Cramer in 1954 from Pinellas County on the Gulf Coast, where demographic changes were underway. In this period, African Americans were still disenfranchised by the state's constitution and discriminatory practices; in the 19th century they had made up most of the Republican Party. Cramer built a different Republican Party in Florida, attracting local white conservatives and transplants from northern and midwestern states. In 1966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election. In 1968 Edward J. Gurney, also a white conservative, was elected as the state's first post-reconstruction Republican US Senator. In 1970 Democrats took the governorship and the open US Senate seat, and maintained dominance for years.\n",
            "Answer: 966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election\n",
            "Answer extracted from: n 1966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election. I\n",
            "\n",
            "Question: Do American historians downplay or attribute the importance of Freemasonry in causing the American Revolution?\n",
            "Passage: The major opponent of Freemasonry was the Roman Catholic Church, so that in countries with a large Catholic element, such as France, Italy, Spain, and Mexico, much of the ferocity of the political battles involve the confrontation between what Davies calls the reactionary Church and enlightened Freemasonry. Even in France, Masons did not act as a group. American historians, while noting that Benjamin Franklin and George Washington were indeed active Masons, have downplayed the importance of Freemasonry in causing the American Revolution because the Masonic order was non-political and included both Patriots and their enemy the Loyalists.\n",
            "Answer: downplay\n",
            "Answer extracted from: ve downplayed \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwY6t1FKhwTK",
        "cellView": "form"
      },
      "source": [
        "#@title padding { form-width: \"25%\" }\r\n",
        "\r\n",
        "# all sequences in train and val sets will be padded with a number of tokens equal to the maximum sentence length \r\n",
        "MAX_LENGTH_PASSAGE = len(max(df_clean['passage'], key=len))   \r\n",
        "passages = [[word_to_idx[el] for el in sentence.split()] for sentence in df_clean['passage']]  # passages extraction\r\n",
        "passages_pad = tf.keras.preprocessing.sequence.pad_sequences(passages, maxlen=MAX_LENGTH_PASSAGE, padding='post') # padding passages\r\n",
        "\r\n",
        "MAX_LENGTH_QUESTION = len(max(df_clean['question'], key=len))   \r\n",
        "questions = [[word_to_idx[el] for el in sentence.split()] for sentence in df_clean['question']]  # questions extraction\r\n",
        "questions_pad = tf.keras.preprocessing.sequence.pad_sequences(questions, maxlen=MAX_LENGTH_QUESTION, padding='post') # padding questions\r\n",
        "\r\n",
        "df_clean['passage_pad'] = list(passages_pad)\r\n",
        "df_clean['question_pad'] = list(questions_pad)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjJaZWZyCmI4"
      },
      "source": [
        "# Skip preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cKOshQJ64TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "7601c533-9201-47c8-bfb1-965fb93bbda4"
      },
      "source": [
        "#@title load/store { form-width: \"25%\" }\r\n",
        "import pickle\r\n",
        "load= False #@param {type: \"boolean\"}\r\n",
        "\r\n",
        "if load:\r\n",
        "    !gcloud config set project feisty-mechanic-221914\r\n",
        "    !gsutil cp gs://squad_squad/df_clean.pkl ./df_clean.pkl\r\n",
        "    !gsutil cp gs://squad_squad/embedding_matrix.pkl ./embedding_matrix.pkl\r\n",
        "    !gsutil cp gs://squad_squad/idx_to_word.pkl ./idx_to_word.pkl\r\n",
        "\r\n",
        "    df_clean = pd.read_pickle(\"df_clean.pkl\")\r\n",
        "    with open('embedding_matrix.pkl', 'rb') as handle:\r\n",
        "        embedding_matrix = pickle.load(handle)\r\n",
        "    with open('idx_to_word.pkl', 'rb') as handle:\r\n",
        "        idx_to_word = pickle.load(handle)\r\n",
        "else:\r\n",
        "    df_clean.to_pickle(\"df_clean.pkl\")\r\n",
        "    with open('embedding_matrix.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "    with open('idx_to_word.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(idx_to_word, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "    \r\n",
        "    from google.colab import auth\r\n",
        "    auth.authenticate_user()\r\n",
        "    \r\n",
        "    !gcloud config set project feisty-mechanic-221914\r\n",
        "    !gsutil cp ./df_clean.pkl gs://squad_squad/df_clean.pkl\r\n",
        "    !gsutil cp ./embedding_matrix.pkl gs://squad_squad/embedding_matrix.pkl\r\n",
        "    !gsutil cp ./idx_to_word.pkl gs://squad_squad/idx_to_word.pkl\r\n",
        "\r\n",
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying file://./df_clean.pkl [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "-\n",
            "Operation completed over 1 objects/1.4 GiB.                                      \n",
            "Copying file://./embedding_matrix.pkl [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 1 objects/63.0 MiB.                                     \n",
            "Copying file://./idx_to_word.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
            "Operation completed over 1 objects/1.1 MiB.                                      \n",
            "Fri Feb 12 16:30:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    37W / 250W |    351MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XrcygkwCfqD"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8T_mnKJii2m",
        "cellView": "form"
      },
      "source": [
        "#@title split { form-width: \"25%\" }\r\n",
        "\r\n",
        "split_value = 0.1 #@param {type:\"number\"} \r\n",
        "val_dim = int(len(df_clean['title'].unique()) * split_value)\r\n",
        "val_titles = np.random.choice(df_clean['title'].unique(), size=val_dim, replace=False)\r\n",
        "passage_length = len(df_clean['passage_pad'][0])\r\n",
        "question_length = len(df_clean['question_pad'][0])\r\n",
        "\r\n",
        "# creating train and val sets\r\n",
        "df_val = df_clean[df_clean['title'].isin(val_titles)]\r\n",
        "df_train = df_clean[~(df_clean['title'].isin(val_titles))]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gbCvH4xjEfo",
        "cellView": "form"
      },
      "source": [
        "#@title model definition { form-width: \"25%\" }\n",
        "\n",
        "def build_model():\n",
        "    input_size=len(idx_to_word)\n",
        "    embedding_dim=100\n",
        "    encoding_units = 256\n",
        "    dropout = 0\n",
        "\n",
        "    input_passage = tf.keras.layers.Input(shape=[None])\n",
        "    input_question = tf.keras.layers.Input(shape=[None])\n",
        "\n",
        "    # EMBEDDING\n",
        "    embedding = tf.keras.layers.Embedding(input_size,\n",
        "                                        embedding_dim,  \n",
        "                                        weights=[embedding_matrix],\n",
        "                                        trainable=False,\n",
        "                                        mask_zero=True\n",
        "                                        )   # trainable param is False because we use pre-trained Glove embeddings, mask_zero param is True because we have padding\n",
        "    embedding_passage = embedding(input_passage)\n",
        "    embedding_question = embedding(input_question)\n",
        "\n",
        "\n",
        "    # ENCODING passage AND question\n",
        "    encoding_passage = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=True))(embedding_passage)   \n",
        "\n",
        "    encoding_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=True))(embedding_question)\n",
        "\n",
        "    # ATTENTION LAYER\n",
        "    #encoded_question = tf.keras.layers.RepeatVector(MAX_LENGTH_PASSAGE)(encoding_question)\n",
        "    query_value_attention_seq = tf.keras.layers.Attention()([encoding_passage, encoding_question])   # out shape: [batch_size, passage_len, encoding_dim]\n",
        "    # reducing over the sequence axis to produce encodings of shape [batch_size, encoding_dimension]\n",
        "    #query_value_attention_seq = tf.keras.layers.GlobalAveragePooling1D()(query_value_attention_seq)\n",
        "    # concatenating passage and question encodings on the sequence length dimension\n",
        "    #encoded_pair = tf.keras.layers.concatenate([encoding_passage, encoding_question],axis=1)\n",
        "    # reducing over the sequence axis to produce encodings of shape [batch_size, encoding_dimension]\n",
        "    #encoded_pair = tf.keras.layers.GlobalAveragePooling1D()(encoded_pair)   \n",
        "    # concatenating passage and question encoding pair with the attention result\n",
        "    #combined = tf.keras.layers.concatenate([encoded_pair, query_value_attention_seq])\n",
        "\n",
        "    combined = tf.keras.layers.concatenate([encoding_passage, query_value_attention_seq])  # tried add but doesnt work\n",
        "\n",
        "    lstm_start = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=True))(combined)\n",
        "\n",
        "\n",
        "    output_start = tf.keras.layers.Dense(1)(lstm_start)\n",
        "    logits = tf.squeeze(output_start, axis=[2]) # shape (batch_size, seq_len)\n",
        "    output_start=tf.keras.layers.Softmax(name=\"answ_start\")(logits)\n",
        "\n",
        "    # output_end= tf.keras.layers.Dense(256)(combined) # this last dense layer outputs the positive class probability\n",
        "    # output_end = tf.keras.layers.Dense(1)(output_end)\n",
        "    # logits = tf.squeeze(output_end, axis=[2]) # shape (batch_size, seq_len)\n",
        "    # output_end=tf.keras.layers.Softmax(name=\"answ_end\")(logits)\n",
        "\n",
        "    # output_start = tf.keras.layers.Dense(1)(query_value_attention_seq) # this last dense layer outputs the positive class probability\n",
        "    # output_start = tf.keras.layers.Reshape((passage_length,))(output_start)\n",
        "    # output_start=tf.keras.layers.Softmax(name=\"answ_start\")(output_start)\n",
        "\n",
        "\n",
        "\n",
        "    #output_start_reshaped = tf.keras.layers.Reshape((passage_length,1))(output_start)\n",
        "\n",
        "    #combined_with_start_probabilities = tf.keras.layers.concatenate([combined, output_start_reshaped])\n",
        "\n",
        "    lstm_end = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=True))(combined)\n",
        "\n",
        "    output_end = tf.keras.layers.Dense(1)(lstm_end)\n",
        "    logits = tf.squeeze(output_end, axis=[2]) # shape (batch_size, seq_len)\n",
        "    output_end=tf.keras.layers.Softmax(name=\"answ_end\")(logits)\n",
        "\n",
        "\n",
        "    # output_end = tf.keras.layers.Dense(1)(lstm_end) # this last dense layer outputs the positive class probability\n",
        "    # output_end = tf.keras.layers.Reshape((passage_length,))(output_end)\n",
        "    # output_end=tf.keras.layers.Softmax(name=\"answ_end\")(output_end)\n",
        "\n",
        "    # out = tf.stack([output_start, output_end],axis=-1)\n",
        "\n",
        "    # model = tf.keras.Model([input_passage,input_question], outputs=[out]) \n",
        "\n",
        "    out = tf.stack([output_start, output_end],axis=-1)\n",
        "    model = tf.keras.Model([input_passage,input_question], outputs=[out])  \n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMALb-9ZrGET",
        "cellView": "form"
      },
      "source": [
        "#@title metrics { form-width: \"25%\" }\r\n",
        "\r\n",
        "def prec(y_true, y_pred):\r\n",
        "    sampled = tf.argmax(y_pred, axis=-1)\r\n",
        "    return 1 - tf.math.count_nonzero(tf.squeeze(tf.cast(y_true, tf.int64)) - sampled) / tf.cast(len(sampled), tf.int64)\r\n",
        "\r\n",
        "def precision_start(y_true, y_pred):\r\n",
        "    return prec(y_true[:,0], y_pred[:,:,0])\r\n",
        "\r\n",
        "def precision_end(y_true, y_pred):\r\n",
        "    return prec(y_true[:,1], y_pred[:,:,1])\r\n",
        "\r\n",
        "def dist(y_true, y_pred):\r\n",
        "    sampled = tf.argmax(y_pred, axis=-1)\r\n",
        "    return tf.reduce_sum(tf.abs(tf.squeeze(tf.cast(y_true, tf.int64)) - sampled)) / tf.cast(len(sampled), tf.int64)\r\n",
        "\r\n",
        "def mean_abs_dist_start(y_true_tuple, y_pred_tuple):\r\n",
        "    return dist(y_true_tuple[:,0], y_pred_tuple[:,:,0])\r\n",
        "    \r\n",
        "def mean_abs_dist_end(y_true_tuple, y_pred_tuple):\r\n",
        "    return dist(y_true_tuple[:,1], y_pred_tuple[:,:,1])\r\n",
        "\r\n",
        "def exact_match(y_true, y_pred):\r\n",
        "    sampled_start = tf.argmax(y_pred[:,:,0], axis=-1)\r\n",
        "    sampled_end = tf.argmax(y_pred[:,:,1], axis=-1)\r\n",
        "    start_diff = tf.cast(tf.math.abs(tf.cast(y_true[:,0], tf.int64) - sampled_start),tf.float32)\r\n",
        "    end_diff = tf.cast(tf.math.abs(tf.cast(y_true[:,1], tf.int64) - sampled_end),tf.float32)\r\n",
        "    count = tf.math.count_nonzero(tf.cast(start_diff + end_diff, tf.int64))\r\n",
        "    return 1 - count / tf.cast(len(y_true), tf.int64)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3tICXuzmwmn",
        "cellView": "form"
      },
      "source": [
        "#@title train {form-width: \"25%\"}\n",
        "\n",
        "cc = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "α = 1\n",
        "β = 0\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    sampled_start = tf.argmax(y_pred[:,:,0], axis=-1)\n",
        "    sampled_end = tf.argmax(y_pred[:,:,1], axis=-1)\n",
        "    precedence_constraint = - tf.math.minimum(sampled_end - sampled_start, \n",
        "                                             tf.zeros(len(sampled_end), dtype=tf.int64)) / tf.cast(len(sampled_end), dtype=tf.int64)\n",
        "    precedence_constraint = tf.cast(precedence_constraint, tf.float32)\n",
        "\n",
        "    start_loss = cc(y_true[:,0], y_pred[:,:,0])\n",
        "    end_loss = cc(y_true[:,1], y_pred[:,:,1])\n",
        "    cross_entropy = start_loss + end_loss \n",
        "\n",
        "    return α*cross_entropy + β*precedence_constraint\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 1000\n",
        "\n",
        "ENABLE_WANDB = False        #@param {type:\"boolean\"}\n",
        "wandb_experiment_name = \"still_plaing_with_layer_norm\"  #@param {type: \"string\"}\n",
        "if ENABLE_WANDB:\n",
        "    !pip install wandb > /dev/null\n",
        "    !wandb login wandb_api_token\n",
        "    import wandb\n",
        "    from wandb.keras import WandbCallback\n",
        "    wandb.init(project=\"SQUAD\", name=wandb_experiment_name)\n",
        "    wandb.config.batch_size = batch_size\n",
        "    wandb.config.epochs = epochs\n",
        "    \n",
        "\n",
        "saveDir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(saveDir):\n",
        "    os.makedirs(saveDir)\n",
        "chkpt = saveDir + '/squad_check.hdf5'\n",
        "\n",
        "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
        "cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "if ENABLE_WANDB:\n",
        "    callbacks = [es_cb, cp_cb, WandbCallback(log_batch_frequency=10)]\n",
        "else:\n",
        "    callbacks = [es_cb, cp_cb]\n",
        "metrics = [precision_start, precision_end, mean_abs_dist_start, mean_abs_dist_end, exact_match]\n",
        "\n",
        "model = build_model()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=metrics)\n",
        "\n",
        "history = model.fit([np.stack(df_train['passage_pad']),np.stack(df_train['question_pad'])],np.stack(df_train['word_idx_answer'].to_numpy()), epochs=epochs,\n",
        "                        callbacks=callbacks, validation_data=([np.stack(df_val['passage_pad']), np.stack(df_val['question_pad'])],np.stack(df_val['word_idx_answer'].to_numpy())),\n",
        "                        batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe41qgu25sjR",
        "cellView": "form"
      },
      "source": [
        "#@title precedence plot {form-width: \"25%\"}\r\n",
        "\r\n",
        "predictions = model.predict([np.stack(df_val['passage_pad']), np.stack(df_val['question_pad'])])\r\n",
        "sampled_start = np.argmax(predictions[:,:,0], axis=-1)\r\n",
        "sampled_end = np.argmax(predictions[:,:,1], axis=-1)\r\n",
        "plt.figure(figsize=(30,30))\r\n",
        "plt.plot(np.stack(df_val['word_idx_answer'])[:,0],np.stack(df_val['word_idx_answer'])[:,1], \".\")\r\n",
        "plt.plot(sampled_start, sampled_end,\"*\")\r\n",
        "\r\n",
        "print(\"end before start ratio\")\r\n",
        "precedence_violation = sum(sampled_end - sampled_start < 0) / len(sampled_end) * 100\r\n",
        "print(precedence_violation)\r\n",
        "if ENABLE_WANDB:\r\n",
        "    wandb.log({\"precedence violation\": wandb.Html(\r\n",
        "        \"<pre>precedence violation: \"+str(precedence_violation)+\" %<pre>\", inject=False)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mXyQL7zHUI0"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4wVWJUwHWeE"
      },
      "source": [
        "#@title evaluation script {form-width: \"25%\"}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}