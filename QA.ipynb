{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-SQuAD-squad/IR-QA/blob/regression/QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmkNAxSnaD9n"
      },
      "source": [
        "#@title Init { form-width: \"25%\" }\r\n",
        "import os\r\n",
        "import random\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "pd.set_option('display.max_colwidth', -1)\r\n",
        "\r\n",
        "# fix random seeds\r\n",
        "seed_value = 42 #@param {type:\"integer\"}\r\n",
        "\r\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "random.seed(seed_value)\r\n",
        "np.random.seed(seed_value)\r\n",
        "\r\n",
        "tf.compat.v1.set_random_seed(seed_value)\r\n",
        "\r\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\r\n",
        "tf.compat.v1.keras.backend.set_session(sess)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El70BLeMa4np"
      },
      "source": [
        "#@title df creation { form-width: \"25%\" }\n",
        "\n",
        "# dataset is copyed to public git repo for fast access within colab \n",
        "!wget 'https://raw.githubusercontent.com/the-SQuAD-squad/data/main/SQuAD/squad1.1.zip'\n",
        "!unzip -o squad1.1.zip\n",
        "\n",
        "with open(\"training_set.json\", \"r\") as f:\n",
        "    json_file = json.load(f)\n",
        "data = json_file[\"data\"]\n",
        "\n",
        "rows = []\n",
        "for document in data:\n",
        "  for par in document['paragraphs']:\n",
        "    for qas in par['qas']:\n",
        "      rows.append({\n",
        "        'id' : qas['id'],\n",
        "        'title': document[\"title\"],\n",
        "        'passage': par['context'],\n",
        "        'question' : qas['question'],\n",
        "        'answer_idx' : (qas['answers'][0]['answer_start'], \n",
        "                    qas['answers'][0]['answer_start'] + len(qas['answers'][0]['text'])),\n",
        "        'answer_text' : qas['answers'][0]['text']\n",
        "      })\n",
        "\n",
        "df_original = pd.DataFrame(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zeiC6RRR7wv"
      },
      "source": [
        "df_original"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feNDF6x2Qeb1"
      },
      "source": [
        "import nltk\r\n",
        "import re \r\n",
        "import math\r\n",
        "\r\n",
        "def preprocess_text(text):\r\n",
        "    \"\"\"\r\n",
        "    Given an iterable containing sentences, pre-process each sentence.\r\n",
        "\r\n",
        "    :param: \r\n",
        "        - text: list of text to be pre-processed (Iterable)\r\n",
        "    :return:\r\n",
        "        - text: pre-processed text (List)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    REPLACE_WITH_SPACE = re.compile(r\"\\n\") \r\n",
        "    text = [REPLACE_WITH_SPACE.sub(\" \", line) for line in text]\r\n",
        "\r\n",
        "    # we don't remove symbols, but just put a space before and after them. We did this because we noticed that Glove contains an embedding also for\r\n",
        "    # them, so, in this way, we are able to split these symbols from the text when computing sentence tokens\r\n",
        "    text = [re.sub(r\"([(.;:!\\'ˈ~?,\\\"(\\[\\])\\\\\\/\\-–\\t```<>_#$€@%*+—°′″“”×’^₤₹‘])\", r' \\1 ', line) for line in text]\r\n",
        "\r\n",
        "    # we noticed that in the text sometimes we find numbers and the following word merged together (ex: 1980february),\r\n",
        "    # so we put a space between the number and the word\r\n",
        "    text = [re.sub(r\"(\\d+)([a-z]+)\", r'\\1 \\2', line) for line in text] \r\n",
        "    text = [re.sub('\\s{2,}', ' ', line.strip()) for line in text]   # replacing more than one consecutive blank spaces with only one of them\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "# Creating a copy of the original dataframe (we do this because we want to be able to compare the results of our processing with the original data)\r\n",
        "df = df_original.copy()\r\n",
        "\r\n",
        "# pre-process passage and question text\r\n",
        "df['passage'] = preprocess_text(df_original['passage'])\r\n",
        "df['question'] = preprocess_text(df_original['question'])\r\n",
        "df['answer_text'] = preprocess_text(df_original['answer_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvz7x1ORmmB"
      },
      "source": [
        "import random as rand\r\n",
        "# Comparing Original and Pre-Processed\r\n",
        "for i in range(3):\r\n",
        "    a = rand.randint(0,1000)\r\n",
        "    print('ORIGINAL AND PREPROCESSED PASSAGE:')\r\n",
        "    print(df_original.iloc[a]['passage'])\r\n",
        "    print(df.iloc[a]['passage'])\r\n",
        "    \r\n",
        "    print()\r\n",
        "    print('ORIGINAL AND PREPROCESSED QUESTION:')\r\n",
        "    print(df_original.iloc[a]['question'])\r\n",
        "    print(df.iloc[a]['question'])\r\n",
        "    print()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXg4fox1SVVo"
      },
      "source": [
        "unwanted_rows = set()   # this set will contain the indices of rows containing errors (thus we will remove these rows from the dataframe)\r\n",
        "unwanted_id = set()    # this set will contain the IDs of rows containing errors \r\n",
        "word_idx = []   # will contain the start and end indices of each answer in the corresponing passage\r\n",
        "impossible_count = 0\r\n",
        "\r\n",
        "for i in df_original.index:\r\n",
        "\r\n",
        "    # extracting one answer and the corresponding passage\r\n",
        "    answer = np.array(df[\"answer_text\"][i].split())\r\n",
        "    passage = np.array(df[\"passage\"][i].split())\r\n",
        "\r\n",
        "    l = len(answer)\r\n",
        "    idx = []    # this list will contain the start and end indices of the answer occurrence in the corresponding passage (these could be more than 1! ex: we find the answer \"rome\" in 2 distinct positions in the same passage)\r\n",
        "    counts = []     # counts will contain how many characters there are before the answer start in the passage\r\n",
        "    char_count = 0\r\n",
        "    for j in range(len(passage)-l+1):   # for each token in the passage, check if the answer starts from that token\r\n",
        "        if (answer == passage[j:j+l]).all():\r\n",
        "            idx.append((j, j+l))    # j is the start_index and j+l is the end_index of the answer in the passage\r\n",
        "            counts.append(char_count)    # char_count is the count of characters before the start of the answer\r\n",
        "        char_count += len(passage[j])\r\n",
        "    if len(counts) == 0:    # no answer found in the passage. Probably there is an error in the dataset (for instance the answer is \"7\", but in the text it is written like \"seven\")\r\n",
        "        unwanted_rows.add(i)\r\n",
        "        unwanted_id.add(df_original[\"id\"][i])\r\n",
        "        word_idx.append((-1, -1))   # stating that there was an error\r\n",
        "        impossible_count+=1\r\n",
        "\r\n",
        "        # printing original question, answer and passage (by using the answer id to find the right dataframe row)\r\n",
        "        print(str(df_original[\"question\"][df_original[\"id\"] == df[\"id\"][i]]))\r\n",
        "        print(str(df_original['passage'][df_original['id'] == df['id'][i]]))\r\n",
        "        print(str(df_original['answer_text'][df_original['id'] == df['id'][i]]))\r\n",
        "        print(\"answer extracted from: {0}\".format((df_original['passage'][i][df_original[\"answer_idx\"][i][0]-3:df_original[\"answer_idx\"][i][1]+3])))\r\n",
        "        print()\r\n",
        "\r\n",
        "    else:   # answer found in the passage\r\n",
        "        # if more than one answer correspondence was found in the passage, we take the one whose start index is nearer the start index given in the dataset \r\n",
        "        # (these 2 starting indices do not match perfectly because we are working on the preoprocessed text, so our starting index is a little bit different from the original)\r\n",
        "        n_spaces_original = df_original[\"passage\"][i][:df_original[\"answer_idx\"][i][0]].count(\" \")    # counting how many spaces there are in the original passage before the answer\r\n",
        "        n_newline_original = df_original[\"passage\"][i][:df_original[\"answer_idx\"][i][0]].count(\"\\n\")    # counting how many newline characters there are in the original passage before the answer\r\n",
        "        s = np.abs(np.array(counts)-(df[\"answer_idx\"][i][0]-n_spaces_original-n_newline_original))\r\n",
        "\r\n",
        "        if (0 not in s) and len(s)>1:   # in this case the answer was found in the passage, but the start index specified in the dataset is not the right one (it indicates a wrong occurrence of the answer)\r\n",
        "            unwanted_rows.add(i)\r\n",
        "            unwanted_id.add(df_original[\"id\"][i])\r\n",
        "            word_idx.append((-1, -1))   # stating that there was an error\r\n",
        "            impossible_count+=1\r\n",
        "\r\n",
        "            print(str(df_original[\"question\"][df_original[\"id\"]==df[\"id\"][i]]))\r\n",
        "            print(str(df_original['passage'][df_original['id'] == df['id'][i]]))\r\n",
        "            print(str(df_original['answer_text'][df_original['id'] == df['id'][i]]))\r\n",
        "            print(\"answer extracted from: {0}\".format((df_original['passage'][i][df_original[\"answer_idx\"][i][0]-3:df_original[\"answer_idx\"][i][1]+3])))\r\n",
        "            print()\r\n",
        "        else:\r\n",
        "            word_idx.append(idx[np.argmin(s)])\r\n",
        "            # print(df_original[\"answer_text\"][i])\r\n",
        "            # print(passage[idx[np.argmin(s)][0]:idx[np.argmin(s)][1]])\r\n",
        "\r\n",
        "print(\"The number of rows that we will remove from the dataframe (because contain errors) are {0}\".format(impossible_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXoOAUUEI1jd"
      },
      "source": [
        "# adding a new column to the dataframe containing the word indices of the answer in the splitted passage\r\n",
        "df[\"word_idx_answer\"] = word_idx\r\n",
        "df[\"passage\"]=df[\"passage\"].str.lower()\r\n",
        "df[\"question\"]=df[\"question\"].str.lower()\r\n",
        "df[\"answer_text\"]=df[\"answer_text\"].str.lower()\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL9cFDAeTdT7"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def build_vocabulary(text):\r\n",
        "    \"\"\"\r\n",
        "    Given a list of words, builds the corresponding word vocabulary and the mappings from words to indices and vice-versa.\r\n",
        "\r\n",
        "    :param: \r\n",
        "        - text: list of words from which we want to build the vocabularies (List)\r\n",
        "    :return:\r\n",
        "        - idx2word: index to word mapping (Dict)\r\n",
        "        - word2idx: word to index mapping (Dict)\r\n",
        "        - set_vocab: set of unique terms that build up the vocabulary\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Creating a set to eliminate repeated words\r\n",
        "    set_vocab = ['<PAD>']+sorted(set(text)) # here we add the padding token as the first element of the set\r\n",
        "\r\n",
        "    # Creating a mapping from unique words to indices\r\n",
        "    word2idx = {u:i for i, u in enumerate(set_vocab)}   # the padding token will have 0 index\r\n",
        "    # Creating a mapping from indices to unique words\r\n",
        "    idx2word = {i:u for i, u in enumerate(set_vocab)}\r\n",
        "\r\n",
        "    return idx2word,word2idx,set_vocab\r\n",
        "\r\n",
        "# Creating a list containing all the passage and question text splitted in words\r\n",
        "text =  ' '.join(np.concatenate((df['passage'],df['question']))).split(' ')\r\n",
        "# Displaying first 100 words\r\n",
        "print(text[:100])\r\n",
        "# calling the build_vocabulary function to obtain the vocab and the mappings\r\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(text)\r\n",
        "\r\n",
        "print('[Debug] Index -> Word vocabulary size: {}'.format(len(idx_to_word)))\r\n",
        "print('[Debug] Word -> Index vocabulary size: {}'.format(len(word_to_idx)))\r\n",
        "print('[Debug] Some words: {}'.format([(idx_to_word[idx], idx) for idx in np.arange(100)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8xiVS-hXc4g"
      },
      "source": [
        "import gensim\r\n",
        "import gensim.downloader as gloader\r\n",
        "\r\n",
        "def load_embedding_model(model_type, embedding_dimension=50):\r\n",
        "    \"\"\"\r\n",
        "    Loads a pre-trained word embedding model via gensim library.\r\n",
        "\r\n",
        "    :params:\r\n",
        "        - model_type: name of the word embedding model to load.\r\n",
        "        - embedding_dimension: size of the embedding space to consider\r\n",
        "\r\n",
        "    :return:\r\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    download_path = \"\"\r\n",
        "\r\n",
        "    # Find the correct embedding model name\r\n",
        "    if model_type.strip().lower() == 'word2vec':\r\n",
        "        download_path = \"word2vec-google-news-300\"\r\n",
        "\r\n",
        "    elif model_type.strip().lower() == 'glove':\r\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\r\n",
        "\r\n",
        "    else:\r\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\r\n",
        "\r\n",
        "    # Check download\r\n",
        "    try:\r\n",
        "        emb_model = gloader.load(download_path)\r\n",
        "    except ValueError as e:\r\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\r\n",
        "        print(\"Word2Vec: 300\")\r\n",
        "        print(\"Glove: 50, 100, 200, 300\")\r\n",
        "        raise e\r\n",
        "\r\n",
        "    return emb_model\r\n",
        "\r\n",
        "\r\n",
        "def check_OOV_terms(embedding_model, word_listing):\r\n",
        "    \"\"\"\r\n",
        "    Checks differences between pre-trained embedding model vocabulary\r\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\r\n",
        "\r\n",
        "    :params:\r\n",
        "        - embedding_model: pre-trained word embedding model (gensim wrapper)\r\n",
        "        - word_listing: dataset specific vocabulary (list)\r\n",
        "\r\n",
        "    :return:\r\n",
        "        - list of OOV terms\r\n",
        "    \"\"\"\r\n",
        "    # Creating a list for the OOV words\r\n",
        "    oov = []\r\n",
        "    for word in word_listing:\r\n",
        "        # Checking if the word is in the embedding_model\r\n",
        "        if word not in embedding_model:\r\n",
        "            oov.append(word)\r\n",
        "    return oov\r\n",
        "\r\n",
        "\r\n",
        "def build_embedding_matrix_w_random(embedding_model, embedding_dimension, word_to_idx, oov_terms):\r\n",
        "    \"\"\"\r\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\r\n",
        "\r\n",
        "    :params: \r\n",
        "        - embedding_model: pre-trained word embedding model (gensim wrapper)\r\n",
        "        - word_to_idx: vocabulary map (word -> index) (dict)\r\n",
        "        - oov_terms: list of OOV terms (list)\r\n",
        "\r\n",
        "    :return\r\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\r\n",
        "    \"\"\"\r\n",
        "    embedding_matrix = []\r\n",
        "    for word in word_to_idx:\r\n",
        "        if word in oov_terms:\r\n",
        "            embedding_matrix.append(np.random.rand(embedding_dimension))\r\n",
        "        else:\r\n",
        "             embedding_matrix.append(embedding_model[word])\r\n",
        "    return np.array(embedding_matrix)\r\n",
        "\r\n",
        "# we used Glove with embedding dimension 100 for our final tests\r\n",
        "embedding_model_type = \"Glove\"\r\n",
        "embedding_dimension = 100\r\n",
        "embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2UcS3aZXoJj"
      },
      "source": [
        "# checking how many OOV terms we have\r\n",
        "oov_terms = check_OOV_terms(embedding_model, word_listing)\r\n",
        "\r\n",
        "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), len(oov_terms)/len(word_listing)*100))\r\n",
        "\r\n",
        "embedding_matrix = build_embedding_matrix_w_random(embedding_model, embedding_dimension, word_to_idx, oov_terms)\r\n",
        "\r\n",
        "print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFUg4VPRXouk"
      },
      "source": [
        "print(oov_terms)   # this was useful to understand if we could improve pre-processing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnNxXuckyguw"
      },
      "source": [
        "for i,question in enumerate(df['question']):\r\n",
        "    if len(question)<=10:\r\n",
        "        print(df_original.iloc[i]) \r\n",
        "        print(df_original['passage'][i])\r\n",
        "        print()\r\n",
        "        unwanted_rows.add(i)\r\n",
        "        unwanted_id.add(df[\"id\"][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmT-17Ydn905"
      },
      "source": [
        "# ERROR CHECK before removing rows\r\n",
        "print(\"Number of errors found: {0}\\n\".format(str(len(unwanted_id))))\r\n",
        "some_error_id = random.sample(unwanted_id, 5)\r\n",
        "for id in some_error_id:\r\n",
        "    row_with_error = df_original[df_original[\"id\"] == id].to_dict(\"list\")\r\n",
        "    print(\"Question: {0}\".format(row_with_error[\"question\"][0]))\r\n",
        "    print(\"Passage: {0}\".format(row_with_error[\"passage\"][0]))\r\n",
        "    print(\"Answer: {0}\".format(row_with_error[\"answer_text\"][0]))\r\n",
        "    print(\"Answer extracted from: {0}\".format(row_with_error[\"passage\"][0][row_with_error[\"answer_idx\"][0][0]-3:row_with_error[\"answer_idx\"][0][1]+3]))\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z2_wz-QzRYy"
      },
      "source": [
        "# Creating a txt file containing IDs of rows with errors for tutors\r\n",
        "with open(\"error IDs.txt\", \"a\") as f:\r\n",
        "    for error_id in unwanted_id:\r\n",
        "        f.write(error_id + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bliwe-ey3_r"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTEBr9XLzSr6"
      },
      "source": [
        "df_clean = df.drop(list(unwanted_rows))\r\n",
        "df_clean = df_clean.reset_index()\r\n",
        "df_clean.to_pickle(\"df_clean.pkl\")\r\n",
        "!zip df_clean.pkl.zip df_clean.pkl\r\n",
        "df_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwY6t1FKhwTK"
      },
      "source": [
        "# PADDING\r\n",
        "# all sequences in train and val sets will be padded with a number of tokens equal to the maximum sentence length \r\n",
        "MAX_LENGTH_PASSAGE = len(max(df_clean['passage'], key=len))   \r\n",
        "passages = [[word_to_idx[el] for el in sentence.split()] for sentence in df_clean['passage']]  # passages extraction\r\n",
        "passages_pad = tf.keras.preprocessing.sequence.pad_sequences(passages, maxlen=MAX_LENGTH_PASSAGE, padding='post') # padding passages\r\n",
        "\r\n",
        "MAX_LENGTH_QUESTION = len(max(df_clean['question'], key=len))   \r\n",
        "questions = [[word_to_idx[el] for el in sentence.split()] for sentence in df_clean['question']]  # questions extraction\r\n",
        "questions_pad = tf.keras.preprocessing.sequence.pad_sequences(questions, maxlen=MAX_LENGTH_QUESTION, padding='post') # padding questions\r\n",
        "\r\n",
        "df_clean['passage_pad'] = list(passages_pad)\r\n",
        "df_clean['question_pad'] = list(questions_pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cKOshQJ64TS"
      },
      "source": [
        "#@title Skip preprocessing\r\n",
        "import pickle\r\n",
        "load= True #@param {type: \"boolean\"}\r\n",
        "\r\n",
        "if load:\r\n",
        "    import os\r\n",
        "    import random\r\n",
        "    import math\r\n",
        "    import numpy as np\r\n",
        "    import tensorflow as tf\r\n",
        "    import json\r\n",
        "    import pandas as pd\r\n",
        "\r\n",
        "    pd.set_option('display.max_colwidth', -1)\r\n",
        "\r\n",
        "    # fix random seeds\r\n",
        "    seed_value = 42 #@param {type:\"integer\"}\r\n",
        "    os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "    random.seed(seed_value)\r\n",
        "    np.random.seed(seed_value)\r\n",
        "    tf.compat.v1.set_random_seed(seed_value)\r\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\r\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\r\n",
        "\r\n",
        "\r\n",
        "    !gcloud config set project feisty-mechanic-221914\r\n",
        "    !gsutil cp gs://squad_squad/df_clean.pkl ./df_clean.pkl\r\n",
        "    !gsutil cp gs://squad_squad/embedding_matrix.pkl ./embedding_matrix.pkl\r\n",
        "    !gsutil cp gs://squad_squad/idx_to_word.pkl ./idx_to_word.pkl\r\n",
        "\r\n",
        "    df_clean = pd.read_pickle(\"df_clean.pkl\")\r\n",
        "    with open('embedding_matrix.pkl', 'rb') as handle:\r\n",
        "        embedding_matrix = pickle.load(handle)\r\n",
        "    with open('idx_to_word.pkl', 'rb') as handle:\r\n",
        "        idx_to_word = pickle.load(handle)\r\n",
        "else:\r\n",
        "    df_clean.to_pickle(\"df_clean.pkl\")\r\n",
        "    with open('embedding_matrix.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "    with open('idx_to_word.pkl', 'wb') as handle:\r\n",
        "        pickle.dump(idx_to_word, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "    \r\n",
        "    from google.colab import auth\r\n",
        "    auth.authenticate_user()\r\n",
        "    !gcloud config set project feisty-mechanic-221914\r\n",
        "    !gsutil cp ./df_clean.pkl gs://squad_squad/df_clean.pkl\r\n",
        "    !gsutil cp ./embedding_matrix.pkl gs://squad_squad/embedding_matrix.pkl\r\n",
        "    !gsutil cp ./idx_to_word.pkl gs://squad_squad/idx_to_word.pkl\r\n",
        "\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8T_mnKJii2m"
      },
      "source": [
        "#@title split { form-width: \"25%\" }\r\n",
        "\r\n",
        "split_value = 0.1 #@param {type:\"number\"} \r\n",
        "val_dim = int(len(df_clean['title'].unique()) * split_value)\r\n",
        "val_titles = np.random.choice(df_clean['title'].unique(), size=val_dim, replace=False)\r\n",
        "passage_length = len(df_clean['passage_pad'][0])\r\n",
        "question_length = len(df_clean['question_pad'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIWeU855kHoj"
      },
      "source": [
        "# creating train and val sets\r\n",
        "df_val = df_clean[df_clean['title'].isin(val_titles)]\r\n",
        "df_train = df_clean[~(df_clean['title'].isin(val_titles))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmgKMaoakvEH"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gbCvH4xjEfo"
      },
      "source": [
        "def build_model():\n",
        "    input_size=len(idx_to_word)\n",
        "    embedding_dim=100\n",
        "    encoding_units = 256\n",
        "    dropout = 0\n",
        "\n",
        "    input_passage = tf.keras.layers.Input(shape=[None])\n",
        "    input_question = tf.keras.layers.Input(shape=[None])\n",
        "\n",
        "    # EMBEDDING\n",
        "    embedding = tf.keras.layers.Embedding(input_size,\n",
        "                                        embedding_dim,  \n",
        "                                        weights=[embedding_matrix],\n",
        "                                        trainable=False,\n",
        "                                        mask_zero=True\n",
        "                                        )   # trainable param is False because we use pre-trained Glove embeddings, mask_zero param is True because we have padding\n",
        "    embedding_passage = embedding(input_passage)\n",
        "    embedding_question = embedding(input_question)\n",
        "\n",
        "\n",
        "    # ENCODING passage AND question\n",
        "    encoding_passage = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=True))(embedding_passage)   \n",
        "\n",
        "    encoding_question = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=True))(embedding_question)\n",
        "\n",
        "    # ATTENTION LAYER\n",
        "    #encoded_question = tf.keras.layers.RepeatVector(MAX_LENGTH_PASSAGE)(encoding_question)\n",
        "    query_value_attention_seq = tf.keras.layers.Attention()([encoding_passage, encoding_question])   # out shape: [batch_size, passage_len, encoding_dim]\n",
        "    # reducing over the sequence axis to produce encodings of shape [batch_size, encoding_dimension]\n",
        "    #query_value_attention_seq = tf.keras.layers.GlobalAveragePooling1D()(query_value_attention_seq)\n",
        "    # concatenating passage and question encodings on the sequence length dimension\n",
        "    #encoded_pair = tf.keras.layers.concatenate([encoding_passage, encoding_question],axis=1)\n",
        "    # reducing over the sequence axis to produce encodings of shape [batch_size, encoding_dimension]\n",
        "    #encoded_pair = tf.keras.layers.GlobalAveragePooling1D()(encoded_pair)   \n",
        "    # concatenating passage and question encoding pair with the attention result\n",
        "    #combined = tf.keras.layers.concatenate([encoded_pair, query_value_attention_seq])\n",
        "\n",
        "    combined = tf.keras.layers.concatenate([encoding_passage, query_value_attention_seq])  # tried add but doesnt work\n",
        "    combined = tf.keras.layers.LayerNormalization()(combined)\n",
        "\n",
        "    lstm_start = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=True))(combined)\n",
        "\n",
        "\n",
        "    output_start = tf.keras.layers.Dense(1)(lstm_start)\n",
        "    logits = tf.squeeze(output_start, axis=[2]) # shape (batch_size, seq_len)\n",
        "    output_start=tf.keras.layers.Softmax(name=\"answ_start\")(logits)\n",
        "\n",
        "    # output_end= tf.keras.layers.Dense(256)(combined) # this last dense layer outputs the positive class probability\n",
        "    # output_end = tf.keras.layers.Dense(1)(output_end)\n",
        "    # logits = tf.squeeze(output_end, axis=[2]) # shape (batch_size, seq_len)\n",
        "    # output_end=tf.keras.layers.Softmax(name=\"answ_end\")(logits)\n",
        "\n",
        "    # output_start = tf.keras.layers.Dense(1)(query_value_attention_seq) # this last dense layer outputs the positive class probability\n",
        "    # output_start = tf.keras.layers.Reshape((passage_length,))(output_start)\n",
        "    # output_start=tf.keras.layers.Softmax(name=\"answ_start\")(output_start)\n",
        "\n",
        "\n",
        "\n",
        "    #output_start_reshaped = tf.keras.layers.Reshape((passage_length,1))(output_start)\n",
        "\n",
        "    #combined_with_start_probabilities = tf.keras.layers.concatenate([combined, output_start_reshaped])\n",
        "\n",
        "    lstm_end = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(encoding_units, \n",
        "                                kernel_initializer='glorot_uniform',\n",
        "                                recurrent_initializer='orthogonal',\n",
        "                                dropout=dropout,\n",
        "                                stateful=False,\n",
        "                                return_sequences=False))(combined)\n",
        "\n",
        "    output_end = tf.keras.layers.Dense(1)(lstm_end)\n",
        "    #logits = tf.squeeze(output_end, axis=[2]) # shape (batch_size, seq_len)\n",
        "    output_end=tf.keras.layers.ReLU(name=\"answ_end\")(output_end)\n",
        "\n",
        "\n",
        "    # output_end = tf.keras.layers.Dense(1)(lstm_end) # this last dense layer outputs the positive class probability\n",
        "    # output_end = tf.keras.layers.Reshape((passage_length,))(output_end)\n",
        "    # output_end=tf.keras.layers.Softmax(name=\"answ_end\")(output_end)\n",
        "\n",
        "    # out = tf.stack([output_start, output_end],axis=-1)\n",
        "\n",
        "    # model = tf.keras.Model([input_passage,input_question], outputs=[out]) \n",
        "\n",
        "    # output start ha [batch, time steps]\n",
        "    # output end ha [batch,1]\n",
        "\n",
        "    output_end_repeated = tf.keras.layers.RepeatVector(passage_length)(output_end)\n",
        "    output_end_repeated = tf.squeeze(output_end_repeated, axis=[-1])\n",
        "    out = tf.stack([output_start, output_end_repeated],axis=-1)\n",
        "\n",
        "    #out = tf.stack([output_start, output_end],axis=-1)\n",
        "    model = tf.keras.Model([input_passage,input_question], outputs=[out])  \n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMALb-9ZrGET"
      },
      "source": [
        "#@title metrics {form-width: \"10%\"}\r\n",
        "\r\n",
        "def prec(y_true, y_pred):\r\n",
        "    sampled = tf.argmax(y_pred, axis=-1)\r\n",
        "    return 1 - tf.math.count_nonzero(tf.cast(y_true, tf.int64) - sampled) / tf.cast(len(sampled), tf.int64)\r\n",
        "\r\n",
        "def precision_start(y_true, y_pred):\r\n",
        "    return prec(y_true[:,0], y_pred[:,:,0])\r\n",
        "\r\n",
        "def precision_end(y_true, y_pred):\r\n",
        "    pred_len = tf.gather(y_pred[:,:,1], tf.cast(y_true[:,0], tf.int64), axis=1)\r\n",
        "    pred_len = tf.cast(pred_len, tf.int64)\r\n",
        "    return 1 - tf.math.count_nonzero(tf.cast(y_true[:,1]-y_true[:,0], tf.int64) - pred_len) / tf.cast(len(y_pred), tf.int64)\r\n",
        "\r\n",
        "def dist(y_true, y_pred):\r\n",
        "    sampled = tf.argmax(y_pred, axis=-1)\r\n",
        "    return tf.reduce_sum(tf.abs(tf.cast(y_true, tf.int64) - sampled)) / tf.cast(len(sampled), tf.int64)\r\n",
        "\r\n",
        "def mean_abs_dist_start(y_true_tuple, y_pred_tuple):\r\n",
        "    return dist(y_true_tuple[:,0], y_pred_tuple[:,:,0])\r\n",
        "    \r\n",
        "def mean_abs_dist_len(y_true, y_pred):\r\n",
        "\r\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\r\n",
        "    return mae(y_true[:,1] - y_true[:,0] +1, y_pred[:,0,1])\r\n",
        "\r\n",
        "def exact_match(y_true, y_pred):\r\n",
        "    \r\n",
        "    sampled_start = tf.argmax(y_pred[:,:,0], axis=-1)\r\n",
        "\r\n",
        "    start_diff = tf.cast(tf.math.abs(tf.cast(y_true[:,0], tf.int64) - sampled_start),tf.float32)\r\n",
        "    \r\n",
        "    span_diff = tf.math.abs(y_true[:,1] - y_true[:,0] + 1 - tf.cast(\r\n",
        "        tf.math.round(y_pred[:,0,1]),tf.float32))\r\n",
        "    \r\n",
        "    count = tf.math.count_nonzero(tf.cast(start_diff + span_diff, tf.int64))\r\n",
        "    return 1 - count / tf.cast(len(y_true), tf.int64)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3tICXuzmwmn",
        "outputId": "7f05fbba-1b46-4be9-951c-00d70c633991"
      },
      "source": [
        "#@title train {form-width: \"10%\"}\n",
        "cc = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "huber = tf.keras.losses.Huber()\n",
        "α = 1\n",
        "β = 1\n",
        "def custom_loss(y_true, y_pred):\n",
        "\n",
        "    sampled_start = tf.argmax(y_pred[:,:,0], axis=-1)\n",
        "\n",
        "    start_crossentropy = cc(y_true[:,0], y_pred[:,:,0])\n",
        "    # difference between span lengths\n",
        "    len_answer_loss = huber(y_true[:,1] - y_true[:,0] + 1, y_pred[:,0,1])\n",
        "\n",
        "    return α*start_crossentropy + β*len_answer_loss\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "\n",
        "ENABLE_WANDB = False        #@param {type:\"boolean\"}\n",
        "wandb_experiment_name = \"still_plaing_with_layer_norm\"  #@param {type: \"string\"}\n",
        "if ENABLE_WANDB:\n",
        "    !pip install wandb > /dev/null\n",
        "    !wandb login wandb_api_token\n",
        "    import wandb\n",
        "    from wandb.keras import WandbCallback\n",
        "    wandb.init(project=\"SQUAD\", name=wandb_experiment_name)\n",
        "    wandb.config.batch_size = batch_size\n",
        "    wandb.config.epochs = epochs\n",
        "    \n",
        "\n",
        "saveDir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(saveDir):\n",
        "    os.makedirs(saveDir)\n",
        "chkpt = saveDir + '/squad_check.hdf5'\n",
        "\n",
        "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
        "cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "if ENABLE_WANDB:\n",
        "    callbacks = [es_cb, cp_cb, WandbCallback(log_batch_frequency=10)]\n",
        "else:\n",
        "    callbacks = [es_cb, cp_cb]\n",
        "\n",
        "model = build_model()\n",
        "tf.keras.backend.clear_session()\n",
        "model.compile(optimizer='adam', \n",
        "              loss=custom_loss,\n",
        "              metrics=[precision_start, mean_abs_dist_len, exact_match])\n",
        "\n",
        "history = model.fit([np.stack(df_train['passage_pad']), np.stack(df_train['question_pad'])],\n",
        "                    np.stack(df_train['word_idx_answer'].to_numpy()), epochs=epochs,\n",
        "                        callbacks=callbacks, validation_data=([np.stack(df_val['passage_pad']), np.stack(df_val['question_pad'])],np.stack(df_val['word_idx_answer'].to_numpy())),\n",
        "                        batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 753/2507 [========>.....................] - ETA: 12:19 - loss: 5.9137 - precision_start: 0.0943 - mean_abs_dist_len: 2.2787 - exact_match: 0.0276"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe41qgu25sjR"
      },
      "source": [
        "#@title precedence plot {form-width: \"20%\"}\r\n",
        "\r\n",
        "predictions = model.predict([np.stack(df_val['passage_pad']), np.stack(df_val['question_pad'])])\r\n",
        "sampled_start = np.argmax(predictions[:,:,0], axis=-1)\r\n",
        "sampled_end = np.argmax(predictions[:,:,1], axis=-1)\r\n",
        "plt.figure(figsize=(30,30))\r\n",
        "plt.plot(np.stack(df_val['word_idx_answer'])[:,0],np.stack(df_val['word_idx_answer'])[:,1], \".\")\r\n",
        "plt.plot(sampled_start, sampled_end,\"*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx8OJpfBIlG6"
      },
      "source": [
        "print(\"end before start ratio\")\r\n",
        "precedence_violation = sum(sampled_end - sampled_start < 0) / len(sampled_end) * 100\r\n",
        "print(precedence_violation)\r\n",
        "if ENABLE_WANDB:\r\n",
        "    wandb.log({\"precedence violation\": wandb.Html(\r\n",
        "        \"<pre>precedence violation: \"+str(precedence_violation)+\" %<pre>\", inject=False)})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}